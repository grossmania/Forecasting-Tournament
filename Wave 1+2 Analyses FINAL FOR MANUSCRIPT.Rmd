---
title: "Wave 1+2 Analyses"
author: "Igor Grossmann"
date: "2/25/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(forecast)
library(psych)
library(tidyverse)
library(stats) #to get p.adjust
library(irr)
library(lme4)
library(ggplot2)
library(tidyr)
library(emmeans)
library(car)
library(jtools)
library(dplyr)
library(ggsci)
library(dplyr)
library(Hmisc)
library(lubridate)
library(statcomp) #to get complexity measures for time series
library(tsibble) #to converte into time series tibble for tidy analyses
#install.packages("CGPfunctions")
library(CGPfunctions) #to graph change in trends over time.
library(partR2) #to get partR2 for LME models
library(moments) #to get skewness
library(ggpubr) #to combine plots
#library(simr) # to simulate power - not useful in posthoc designs
library(rstanarm) #to get bayesian equivalent of difference tests
library(bayestestR) #to get BayesF factor from BIC scores of different lmer models
options(max.print = 20000, scipen = 1000)

```

```{r setup working directory}

setwd("~/GitHub/Forecasting-Tournament") #igor's working directory

```

```{r Import Data}

dat <- read.csv("dat_for_analyses.csv", stringsAsFactors = FALSE)
dat_long <- read.csv("dat_long.csv", stringsAsFactors = FALSE)

```

```{r get simulated benchmark data & add RW to data}
#add simulation benchmarks
load("sim/BenchmarkData_Combined.RData")

sim.w1 <- Stats_all_benchmarks_raw
sim.w1$Wave <-"First Tournament (May 2020)"
sim.w1 <- subset(sim.w1,source!="Experts"&source!="Lay People")
sim.w1$response <- sim.w1$Mean
sim.w1$lower.CL <- sim.w1$CI_L
sim.w1$upper.CL <- sim.w1$CI_U
sim.w1$Type[sim.w1$source=="Benchmark 1"]<-"Historic Mean"
sim.w1$Type[sim.w1$source=="Benchmark 2"]<-"Random Walk"
sim.w1$Type[sim.w1$source=="Benchmark 3"]<-"Linear Regression"

load("sim/BenchmarkData_Combined_W2.RData")
sim.w2<-Stats_all_benchmarks_raw_w2
sim.w2$Wave<-"Second Tournament (Nov 2020)"
sim.w2<-subset(sim.w2,source!="ExpertsW2")
sim.w2$response<-sim.w2$Mean
sim.w2$lower.CL<-sim.w2$CI_L
sim.w2$upper.CL<-sim.w2$CI_U
sim.w2$Type[sim.w1$source=="Benchmark 1"]<-"Historic Mean"
sim.w2$Type[sim.w1$source=="Benchmark 2"]<-"Random Walk"
sim.w2$Type[sim.w1$source=="Benchmark 3"]<-"Linear Regression"
#get simulation-based random walk cut-offs - to be used for inspection of top teams

#ADD PART how to use RW SIM scores per domain per wave to get the cutoff scores.

##
#subset benchmark, first
sim.w1.rw <- sim.w1 %>% 
  filter(Type == 'Random Walk') %>% 
  mutate(rw.MASE.w1 = response) %>% 
  dplyr::select(domain, rw.MASE.w1)

sim.w2.rw <- sim.w2 %>% 
  filter(Type == 'Random Walk') %>% 
  mutate(rw.MASE.w2 = response) %>% 
  dplyr::select(domain, rw.MASE.w2)

##

## add to the datafile
dat <- dat %>% 
  left_join(sim.w1.rw)

dat <- dat %>% 
  left_join(sim.w2.rw)

## create cut-offs
dat$compare_to_naive_rwf_MASE<-NA #first set to NA
dat<-dat %>% #create difference score between MASE of estimate and RW MASE
mutate(diff_to_naive_rwf_MASE = case_when(
      phase==1 ~ MASE1_w1 - rw.MASE.w1 ,
      phase==2 ~ MASE1_w2 - rw.MASE.w2 
    ))

#check number of NAs
#View(dat[is.na(dat$diff_to_naive_rwf_MASE),c("ResponseId", "team_name", "domain", "compare_to_naive_rwf","phase", "Month.1" ,
#                                            "Month.2" ,"Month.3" ,"Month.4" ,"Month.5" ,"Month.6","Month.7",
#                                           "Month.8" ,"Month.9" ,"Month.10" ,"Month.11" ,"Month.12","MASE1_w2", "rw.MASE.w2")]) 

dat<-dat %>% #use the diff score values to calculate the cut offs (for graphs) later on 
mutate(compare_to_naive_rwf_MASE = case_when(
      diff_to_naive_rwf_MASE < 1 ~ "Below Random Walk",
      diff_to_naive_rwf_MASE == 1 ~ "Equal to Random Walk",
      diff_to_naive_rwf_MASE > 1 ~ "Above Random Walk"
    ))
#cross-check data for NAs
#View(dat[is.na(dat$compare_to_naive_rwf_MASE),c("ResponseId", "team_name", "domain", "compare_to_naive_rwf","phase","Month.1" ,
#                                            "Month.2" ,"Month.3" ,"Month.4" ,"Month.5" ,"Month.6","Month.7",
#                                            "Month.8" ,"Month.9" ,"Month.10" ,"Month.11" ,"Month.12" ,                                           "MASE1_w1", "rw.MASE.w1")]) 


```

```{r set subsets of data for analyses}
# dataset that only includes academic predictions and those who provided open-ended data
academic_only <- filter(dat, isExpert == 1 )

# View(dat[,c("ResponseId", "team_name", "isExpert", "MASE1_w2", "MASE1_w2")])
#note that missing NAs for isExpert at the end of the file is by design,  these are naÃ¯ve benchmark estimates

#datasets that are filtered by phase (1 = May submission, 2 = November submission)
phase1 <- filter(dat, phase == 1)
phase2 <- filter(dat, phase == 2)

# Phase 1 & 2 further filtered to only include academics won't be necessary once we have updated objective data
phase1_exp <- filter(phase1, isExpert == 1)
phase2_exp <- filter(phase2, isExpert == 1)

objective <- dat %>% 
  filter(Method == "Objective", phase == 1) %>% 
  dplyr::select(domain:Month.12)

```

```{r create subsets for separate visualizations}

# Rank order the performance of all teams for each domain using MASE scores - academics only
t1.academ.sorted <- phase1_exp %>%
      arrange(domain, MASE1_w1) %>%
      group_by(domain) %>% 
      mutate(Rank = row_number()) %>% 
      add_count(name="Nteams") %>% 
      dplyr::select(team_name, domain, Rank, Nteams, Method.code, Month.1:Month.12,mean_abs_error_w1,MASE1_w1)

# Create intuitive labels for domains
t1.academ.sorted$Domains[t1.academ.sorted$domain=="eafric"]<-"Explicit African American Bias"
t1.academ.sorted$Domains[t1.academ.sorted$domain=="easian"]<-"Explicit Asian American Bias"
t1.academ.sorted$Domains[t1.academ.sorted$domain=="egend"]<-"Explicit Gender-Career Bias"
t1.academ.sorted$Domains[t1.academ.sorted$domain=="iafric"]<-"Implicit African American Bias"
t1.academ.sorted$Domains[t1.academ.sorted$domain=="iasian"]<-"Implicit Asian American Bias"
t1.academ.sorted$Domains[t1.academ.sorted$domain=="igend"]<-"Implicit Gender-Career Bias"
t1.academ.sorted$Domains[t1.academ.sorted$domain=="ideoldem"]<-"Ideological Preferences for Democrats"
t1.academ.sorted$Domains[t1.academ.sorted$domain=="ideolrep"]<-"Ideological Preferences for Republicans"
t1.academ.sorted$Domains[t1.academ.sorted$domain=="lifesat"]<-"Life Satisfaction"
t1.academ.sorted$Domains[t1.academ.sorted$domain=="negaffect"]<-"Negative Affect in Social Media"
t1.academ.sorted$Domains[t1.academ.sorted$domain=="posaffect"]<-"Positive Affect in Social Media"
t1.academ.sorted$Domains[t1.academ.sorted$domain=="polar"]<-"Political Polarization"

# Compute average accuracy for each domain - non-academic only
t1.nonacadem.av.sorted <- phase1 %>% 
  filter(isExpert.factor == 'Prolific') %>%                                                               dplyr::select(team_name,domain,Month.1:Month.12,mean_abs_error_w1,MASE1_w1,Method.code) %>% 
  group_by(domain) %>% 
  summarise(across(where(is.numeric), mean)) %>% 
  arrange(domain,MASE1_w1) %>% 
  mutate(team_name="average non-academic")

# Compute median accuracy for each domain - non-academic only
t1.nonacadem.median.sorted<- phase1 %>% 
  filter(isExpert.factor == 'Prolific') %>%                                                               dplyr::select(team_name,domain,Month.1:Month.12,mean_abs_error_w1,MASE1_w1,,Method.code) %>% 
  group_by(domain) %>% 
  summarise(across(where(is.numeric), median)) %>% 
  arrange(domain,MASE1_w1) %>% 
  mutate(team_name="median non-academic")

# Compute best prediction for each of the domains - in other words prediction with the lowest MASE scores - non academics
t1.nonacadem.best.sorted <- phase1 %>% 
  filter(isExpert.factor == 'Prolific') %>%                                                               dplyr::select(team_name,domain,Month.1:Month.12,mean_abs_error_w1,MASE1_w1,Method.code) %>% 
  group_by(domain) %>% 
  summarise(across(where(is.numeric), min)) %>% 
  arrange(domain, MASE1_w1) %>% 
  mutate(team_name = "top non-academic")

# Best predictions for academics by domain
t1.academ.best.sorted <- phase1 %>% 
  filter(isExpert.factor == 'Academic') %>%                                                               dplyr::select(team_name,domain,Month.1:Month.12,mean_abs_error_w1,MASE1_w1,Method.code) %>% 
  group_by(domain) %>% 
  summarise(across(where(is.numeric), min)) %>% 
  arrange(domain,MASE1_w1) %>% 
  mutate(team_name="top academic")

# Combine the two datasets.
t1.top.scores <- rbind(t1.academ.best.sorted, t1.nonacadem.best.sorted) %>% 
  arrange(domain, MASE1_w1)
#so, only for life satisfaction and polarization, best academic was better than best non-academic. For all other domains, non-academics were in fact better (but note that the sample of non-academic was larger)

#what is the percentage of academics and lay people, respectively, who were below 1 on MASE?

t1.scores <- rbind(t1.academ.sorted, t1.nonacadem.median.sorted)
# write.csv(t1.scores,"wave1.scores.csv") 

# Rank order the performance of all teams for each domain using MASE scores - academics only
t2.academ.sorted <- academic_only %>% 
  filter(!(phase == 1 & revised == 1)) %>% 
  arrange(domain, MASE1_w2) %>%
  group_by(domain) %>% 
  mutate(Rank = row_number()) %>% 
  add_count(name="Nteams") %>%                                                                            dplyr::select(team_name,domain,Rank,Nteams,Method.code,phase,revised,Month.7:Month.12,mean_abs_error_w2,MASE1_w2)

# Create intuitive labels for each domain
t2.academ.sorted$Domains[t2.academ.sorted$domain=="eafric"]<-"Explicit African American Bias"
t2.academ.sorted$Domains[t2.academ.sorted$domain=="easian"]<-"Explicit Asian American Bias"
t2.academ.sorted$Domains[t2.academ.sorted$domain=="egend"]<-"Explicit Gender-Career Bias"
t2.academ.sorted$Domains[t2.academ.sorted$domain=="iafric"]<-"Implicit African American Bias"
t2.academ.sorted$Domains[t2.academ.sorted$domain=="iasian"]<-"Implicit Asian American Bias"
t2.academ.sorted$Domains[t2.academ.sorted$domain=="igend"]<-"Implicit Gender-Career Bias"
t2.academ.sorted$Domains[t2.academ.sorted$domain=="ideoldem"]<-"Ideological Preferences for Democrats"
t2.academ.sorted$Domains[t2.academ.sorted$domain=="ideolrep"]<-"Ideological Preferences for Republicans"
t2.academ.sorted$Domains[t2.academ.sorted$domain=="lifesat"]<-"Life Satisfaction"
t2.academ.sorted$Domains[t2.academ.sorted$domain=="negaffect"]<-"Negative Affect in Social Media"
t2.academ.sorted$Domains[t2.academ.sorted$domain=="posaffect"]<-"Positive Affect in Social Media"
t2.academ.sorted$Domains[t2.academ.sorted$domain=="polar"]<-"Political Polarization"

# write.csv(t2.academ.sorted,"wave2.scores.csv") 

# Create intuitive labels for each domain
objective$Domains[objective$domain=="eafric"]<-"Explicit African American Bias"
objective$Domains[objective$domain=="easian"]<-"Explicit Asian American Bias"
objective$Domains[objective$domain=="egend"]<-"Explicit Gender-Career Bias"
objective$Domains[objective$domain=="iafric"]<-"Implicit African American Bias"
objective$Domains[objective$domain=="iasian"]<-"Implicit Asian American Bias"
objective$Domains[objective$domain=="igend"]<-"Implicit Gender-Career Bias"
objective$Domains[objective$domain=="ideoldem"]<-"Ideological Preferences for Democrats"
objective$Domains[objective$domain=="ideolrep"]<-"Ideological Preferences for Republicans"
objective$Domains[objective$domain=="lifesat"]<-"Life Satisfaction"
objective$Domains[objective$domain=="negaffect"]<-"Negative Affect in Social Media"
objective$Domains[objective$domain=="posaffect"]<-"Positive Affect in Social Media"
objective$Domains[objective$domain=="polar"]<-"Political Polarization"

```


```{r create a file to share with teams to announce how they did}

# Create intuitive labels for each month
t1.academ.sorted <- t1.academ.sorted %>% rename(MASE=MASE1_w1,MAE=mean_abs_error_w1,
                                              May2020=Month.1,
                                              June2020=Month.2,
                                              July2020=Month.3,
                                              August2020=Month.4,
                                              Sept2020=Month.5,
                                              Oct2020=Month.6,
                                              Nov2020=Month.7,
                                              Dec2020=Month.8,
                                              Jan2021=Month.9,
                                              Feb2021=Month.10,
                                              March2021=Month.11,
                                              April2021=Month.12)

# Create tournament length variable
t1.academ.sorted$Tournament<-"May - 12-months"

# Create intuitive labels for each month
t2.academ.sorted <- t2.academ.sorted %>% rename(MASE=MASE1_w2,MAE=mean_abs_error_w2,
                                              Nov2020=Month.7,
                                              Dec2020=Month.8,
                                              Jan2021=Month.9,
                                              Feb2021=Month.10,
                                              March2021=Month.11,
                                              April2021=Month.12)

# Create tournament length variable
t2.academ.sorted$Tournament<-"November - 6-months"

# Create intuitive labels
objective <- objective %>% rename(May2020=Month.1,
                                              June2020=Month.2,
                                              July2020=Month.3,
                                              August2020=Month.4,
                                              Sept2020=Month.5,
                                              Oct2020=Month.6,
                                              Nov2020=Month.7,
                                              Dec2020=Month.8,
                                              Jan2021=Month.9,
                                              Feb2021=Month.10,
                                              March2021=Month.11,
                                              April2021=Month.12)

objective$Tournament<-"Ground truth marker"

# Combine all and drop unnecessary variables
results<-rbind(t1.academ.sorted, t2.academ.sorted,objective) %>% 
  ungroup() %>% 
  dplyr::select(-domain,-Method.code, -(phase:revised)) 

# Arrange by Tournament and then move Domains and Tournament columns to the beginning of the dataset and all numeric variables after that.
results <- results %>% 
  arrange(Tournament) %>% 
  relocate(where(is.numeric), .after = where(is.character))


# write.csv(results,"final.results.csv")

```


```{r visualize top performers}

#ANALYSES IN THIS SECTION ARE IN PART REPORTED IN THE SUPPLEMENT WENN DESCRIBING TOP PERFORMERS
#OTHER ANALYSES ARE JUST ADDED FOR AN INTERESTED READER, BUT DID NOT MAKE IT IN THE THE PAPER

pd <- position_dodge(0.7) # move them .07 to the left and right
labels<-c(
  eafric = "Exp. African\n-Am. Bias",
  easian = "Exp. Asian\n-Am. Bias",
  egend = "Exp. \nGender Bias",
  iafric = "Imp. African\n-Am. Bias",
  iasian = "Imp. Asian\n-Am. Bias",
  ideoldem = "Dem.\nSupport",
  ideolrep ="Rep.\nSupport",
  igend = "Imp.\nGender Bias",
  lifesat = "Life\nSatisfaction",
  negaffect = "Negative\nAffect",
  polar = "Polit.\nPolarization",
  posaffect = "Positive\nAffect")

#T1


#who won? - Which of the teams of academics made the best predictions for each of the domains? In other words we identify a winner for each of the domains based on the MASE score (so 12 academic winners in total) 
top.1.MASE.t1 <- phase1 %>% 
  filter(isExpert.factor == 'Academic')  %>%
  arrange(domain,MASE1_w1) %>% 
    group_by(team_name) %>%  mutate(n_domains = n()) %>% 
  group_by(domain) %>% 
  dplyr::slice_head(n = 1) %>% dplyr::select(team_name,mean_abs_error_w1,n_domains,MASE1_w1,Month.1:Month.12,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise) %>%           arrange(MASE1_w1)

# write.csv(top.1.MASE.t1,"top.t1.csv")

# median MASE by domain?
median.MASE.t1 <- phase1 %>% 
  filter(isExpert.factor == 'Academic')  %>%
  arrange(domain) %>%
  group_by(domain) %>% 
  dplyr::summarize(MASE_med = median(MASE1_w1)) %>% 
  dplyr::select(domain,MASE_med) %>% 
  arrange(MASE_med)

# write.csv(median.MASE.t1,"medianMASE.t1.csv")

#examine top 5 - Top 5 winning teams for each domain based on MASE score.
top.5.MASE.t1 <- phase1 %>% 
  filter(isExpert.factor == 'Academic')  %>%
  arrange(domain,MASE1_w1) %>%
  group_by(team_name) %>%  mutate(n_domains = n()) %>% 
  group_by(domain) %>% 
  dplyr::slice_head(n = 5) %>% dplyr::select(team_name,MASE1_w1,domain,compare_to_naive_linear_MASE,compare_to_naive_rwf_MASE,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise,n_domains)

# Visualize top 5 winners (using MASE scores) by domain and approach.
#### Fig. S4. in manuscript
top.5.MASE.t1  %>% 
  ggplot(aes(x=domain, y=MASE1_w1, colour=Method.code)) +  
  geom_point(size=3, position=pd, alpha = .5) + 
  scale_x_discrete(labels=labels, name="") + 
  geom_hline(yintercept =1, linetype='dashed',color='red',14) +                                           theme(legend.position="top") + 
  scale_colour_aaas(name="Approach") + 
  ylab("MASE")

# Proportion of top 5 predictions by method used across all domains
proportions(xtabs( ~ Method.code,top.5.MASE.t1))*100 #in total

# Proportion of top 5 predictions by method used by domain
proportions(xtabs( ~ domain+Method.code,top.5.MASE.t1),"domain")*100 #by domain

# Visualize top 5 predictions for each domain as they compare to baseline models such as naive linear model and random walk. 
#### Fig. S5. in manuscript

top.5.MASE.t1  %>%
  ggplot(aes(x=domain, y=MASE1_w1, colour=compare_to_naive_linear_MASE, shape =compare_to_naive_rwf_MASE)) +  
  geom_point(size=3, position=pd, alpha = .5) + 
  scale_x_discrete(labels=labels, name="") + 
  geom_hline(yintercept =1, linetype='dashed', color='red', 14) +                                         theme(legend.position="top") + 
  scale_colour_d3(name="Compared to\nLinear Model") +                                                     scale_shape_discrete(name="Compared to\nRandom Walk") + 
  ylab("MASE")


# Visualize top 5 predictions for each domain by discipline
#### Fig. S6. in manuscript

top.5.MASE.t1 %>%  
  ggplot(aes(x=domain, y=MASE1_w1, colour=discipline)) +  
  geom_point(size=3, position=pd, alpha = .5) + 
  scale_x_discrete(labels=labels, name="") + 
  geom_hline(yintercept =1, linetype='dashed', color='red', 14) +                                         theme(legend.position="top") + 
  scale_colour_d3(name="Field") + 
  ylab("MASE")

# Proportion of top 5 predictions for each domain across all domains (so 60 predictions)
proportions(xtabs( ~ discipline, top.5.MASE.t1))*100 #in total

# Proportion of top 5 predictions for each domain by domains
proportions(xtabs( ~ domain+discipline,top.5.MASE.t1),"domain")*100 #by domain

# Visualize top 5 predictions for each domain as a function of previous forecasting experience
#### Fig. S7. in manuscript

top.5.MASE.t1 %>%  
  ggplot(aes(x=domain, y=MASE1_w1, colour=as.factor(previous_tournament.coded))) +  
  geom_point(size=3, position=pd, alpha = .5) + 
  scale_x_discrete(labels=labels, name="") +                
  geom_hline(yintercept =1, linetype='dashed', color='red', 14) +                                         theme(legend.position="top") + 
  scale_colour_d3(name="Prior Forecasting Experience")+ylab("MASE")

# Proportion of top 5 estimates for each domain as a function of previous forecasting experience
proportions(xtabs( ~ previous_tournament.coded,top.5.MASE.t1))*100 #in total

# Proportion of top 5 estimates for each domain as a function of previous forecasting experience for academics only
proportions(xtabs( ~ previous_tournament.coded,phase1 %>% filter(isExpert.factor == 'Academic') ))*100 #baserate of prior experience to compare to top 5

# Proportion of top 5 estimates for each domain as a function of previous forecasting experience for by domain
proportions(xtabs( ~ domain+previous_tournament.coded,top.5.MASE.t1),"domain")*100 #by domain


# Visualize the size of top 10 teams by domain for academics only
phase1 %>% 
  filter(isExpert.factor == 'Academic')  %>%
   arrange(domain,MASE1_w1) %>% 
   group_by(domain) %>% 
   dplyr::slice_head(n = 10) %>% dplyr::select(team_name,MASE1_w1,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise) %>%
  ggplot(aes(x = domain, y = team_size.coded)) +
  stat_summary(fun.data="mean_cl_boot",  position=pd) + 
  theme_minimal(base_size = 14) +
  theme(legend.position="bottom") + 
  scale_x_discrete(labels=labels, name="") +
  labs(colour = "Approach",fill="Approach", x="",y="Size of Top 10 Teams (M +/- 95%CI)")
 
# Visualize the complexity of the predictions for top 10 teams by domain for academics only
phase1 %>% 
  filter(isExpert.factor == 'Academic')  %>%
  arrange(domain,MASE1_w1) %>% 
  group_by(domain) %>% 
  dplyr::slice_head(n = 10) %>%                                                                           dplyr::select(team_name,MASE1_w1,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise) %>%
  ggplot(aes(x = domain, y = Method.complex)) +
  stat_summary(fun.data="mean_cl_boot",  position=pd)+theme_minimal(base_size = 14) +
  theme(legend.position="bottom") +
  scale_x_discrete(labels=labels, name="") +
  labs(colour = "Approach",fill="Approach", x="",y="Model complexity (M +/- 95%CI)")

# Visualize percentage of females in each team for top 10 teams by domain for academics only
phase1 %>% 
  filter(isExpert.factor == 'Academic')  %>%
  arrange(domain,MASE1_w1) %>%
  group_by(domain) %>% 
  dplyr::slice_head(n = 10) %>%                                                                           dplyr::select(team_name,MASE1_w1,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise,team_gender,team_education,team_Age,non_US ) %>%
  ggplot(aes(x = domain, y = team_gender)) +
  stat_summary(fun.data="mean_cl_boot",  position=pd)+theme_minimal(base_size = 14) +
  theme(legend.position="bottom") +
  scale_x_discrete(labels=labels, name="")+
  labs(colour = "Approach",fill="Approach", x="",y="% Female per Team (M +/- 95%CI)")

# Visualize education for top 10 teams by domain for academics only
phase1 %>% 
  filter(isExpert.factor == 'Academic')  %>%
  arrange(domain,MASE1_w1) %>% 
  group_by(domain) %>% 
  dplyr::slice_head(n = 10) %>% dplyr::select(team_name,MASE1_w1,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise,team_gender,team_education,team_Age,non_US ) %>%
  ggplot(aes(x = domain, y = team_education)) +
  stat_summary(fun.data="mean_cl_boot",  position=pd)+theme_minimal(base_size = 14) +
  theme(legend.position="bottom") + 
  scale_x_discrete(labels=labels, name="") +
  labs(colour = "Approach",fill="Approach", x="",y=" (M +/- 95%CI)")

# Visualize team age for top 10 teams by domain for academics only
phase1 %>% 
  filter(isExpert.factor == 'Academic')  %>%
  arrange(domain,MASE1_w1) %>%group_by(domain) %>% 
  dplyr::slice_head(n = 10) %>%                                                                           dplyr::select(team_name,MASE1_w1,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise,team_gender,team_education,team_Age,non_US ) %>%
  ggplot(aes(x = domain, y = team_Age)) +
  stat_summary(fun.data="mean_cl_boot",  position=pd)+theme_minimal(base_size = 14) +
  theme(legend.position="bottom") +scale_x_discrete(labels=labels, name="") +
  labs(colour = "Approach",fill="Approach", x="",y="% Average Team Age (M +/- 95%CI)")


# Visualize percentage of non-us team members for 10 teams by domain for academics only
phase1 %>% 
  filter(isExpert.factor == 'Academic')  %>%
  arrange(domain,MASE1_w1) %>% 
  group_by(domain) %>% 
  dplyr::slice_head(n = 10) %>%
  dplyr::select(team_name,MASE1_w1,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise,team_gender,team_education,team_Age,non_US )%>%
  ggplot(aes(x = domain, y = non_US)) +
  stat_summary(fun.data="mean_cl_boot", position=pd)+theme_minimal(base_size = 14) +
  theme(legend.position="bottom") + 
  scale_x_discrete(labels=labels, name="") +
  labs(colour = "Approach",fill="Approach", x="",y="% Non-US per Team (M +/- 95%CI)")

## comparison to lay people

## Compare performance to naive RW between academics and lay people across domains.
proportions(xtabs( ~ compare_to_naive_rwf_MASE+isExpert.factor,phase1),"isExpert.factor")*100 #

## Compare performance to naive RW between academics and lay people across domains using chi squared test
chisq.test(xtabs( ~ compare_to_naive_rwf_MASE+isExpert.factor,phase1))

## Compare performance to naive RW by method across domains using chi squared test
chisq.test(xtabs( ~ compare_to_naive_rwf_MASE+Method.code,subset(phase1, compare_to_naive_rwf_MASE!="Equal to Naive rwf"))) #exclude equal as it is negligible and screws up calculation

## Compare performance to naive linear between academics and lay people across domains
proportions(xtabs( ~ compare_to_naive_linear_MASE+isExpert.factor,phase1),"isExpert.factor")*100 #

## Compare performance to naive linear between academics and lay people across domains using chi square test.
chisq.test(xtabs( ~ compare_to_naive_linear_MASE+isExpert.factor,phase1))

## Comparison by method among academics 

## Compare performance against Naive RW by method
proportions(xtabs( ~ compare_to_naive_rwf_MASE+Method.code, phase1), "Method.code")*100 #
chisq.test(xtabs( ~ compare_to_naive_rwf_MASE+Method.code,phase1))


#chisq.test(xtabs( ~ compare_to_naive_rwf_MASE+Method.code,subset(phase1, compare_to_naive_rwf_MASE!="Equal to Naive rwf")))

## Compare performance against Naive RW by method - just for academics
chisq.test(xtabs( ~ compare_to_naive_rwf_MASE+Method.code,phase1_exp))

#chisq.test(xtabs( ~ compare_to_naive_rwf_MASE+Method.code,subset(phase1_exp, compare_to_naive_rwf_MASE!="Equal to Naive rwf")))

# Compare performance against Naive linear by method
proportions(xtabs( ~ compare_to_naive_linear_MASE+Method.code,phase1),"Method.code")*100 #

# Compare performance against Naive linear by method using chi square
chisq.test(xtabs( ~ compare_to_naive_linear_MASE+Method.code,phase1))

# Compare performance against Naive linear by method using chi square just academics
chisq.test(xtabs( ~ compare_to_naive_linear_MASE+Method.code,phase1_exp)) #just comparison of academics

##PHASE 2

#who won? - identify top performers for each domain using MASE scores in phase 2.
top.1.MASE.t2 <- academic_only  %>% 
  filter(!(phase == 1 & revised == 1)) %>% 
  arrange(domain, MASE1_w2) %>% 
  group_by(team_name) %>%  mutate(n_domains = n()) %>% 
  group_by(domain) %>% 
  dplyr::slice_head(n = 1) %>%                                                  dplyr::select(domain,mean_abs_error_w2,n_domains,MASE1_w2,team_name,mean_abs_percent_error_w2,compare_to_naive_linear_MASE_w2,compare_to_naive_rwf_MASE,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise,phase,revised)
  
# write.csv(top.1.MASE.t2,"top.t2.csv")

#median MASE by domain? - compute median accuracy by domain for phase 2.
median.MASE.t2 <- academic_only  %>% 
  filter(!(phase == 1 & revised == 1)) %>%
  arrange(domain) %>% 
  group_by(domain) %>% 
  dplyr::summarize(MASE_med = median(MASE1_w2)) %>% 
  dplyr::select(domain,MASE_med) %>% 
  arrange(MASE_med)

#write.csv(median.MASE.t2,"medianMASE.t2.csv")

#examine top 5 - top 5 performers by domain using MASE scores for academics in phase 2.
top.5.MASE.t2 <- academic_only %>% 
  filter(!(phase == 1 & revised == 1)) %>% 
  arrange(domain,MASE1_w2) %>% 
    group_by(team_name) %>%  mutate(n_domains = n()) %>% 
   group_by(domain) %>% 
  dplyr::slice_head(n = 5) %>% dplyr::select(team_name,MASE1_w2,domain,n_domains,compare_to_naive_linear_MASE_w2,compare_to_naive_rwf_MASE,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise,phase,revised)
  
# Visualize top 5 performers as a function of method used.
#### Fig. S8. in manuscript

top.5.MASE.t2 %>%  
  ggplot(aes(x=domain, y=MASE1_w2, colour=Method.code)) +  
  geom_point(size=3, position=pd, alpha = .5) + 
  scale_x_discrete(labels=labels, name="") +
  geom_hline(yintercept =1, linetype='dashed', color='red', 14) +                                         theme(legend.position="top") +
  scale_colour_aaas(name="Approach") +
  ylab("MASE")

# proportion of different methods used across the top 5 performance across all domains. 
proportions(xtabs( ~ Method.code,top.5.MASE.t2))*100 #in total

# proportion of different methods used across the top 5 performance by domain
proportions(xtabs( ~ domain+Method.code,top.5.MASE.t2),"domain")*100 #by domain


# Top 5 Performance compared against naive linear and random walk by domain.
#### Fig. S9. in manuscript

top.5.MASE.t2 %>% 
  ggplot(aes(x=domain, y=MASE1_w2, colour=compare_to_naive_linear_MASE_w2, shape =compare_to_naive_rwf_MASE )) +  
  geom_point(size=3, position=pd, alpha = .5) + 
  scale_x_discrete(labels=labels, name="") + 
  geom_hline(yintercept =1, linetype='dashed', color='red', 14) +                                         theme(legend.position="top")+
  scale_colour_d3(name="Compared to\nLinear Model") + 
  scale_shape_discrete(name="Compared to\nRandom Walk") +
  ylab("MASE")

# Top 5 Performance by discipline and domain.
#### Fig. S10. in manuscript

top.5.MASE.t2 %>%  
  ggplot(aes(x=domain, y=MASE1_w2, colour=discipline)) +  
  geom_point(size=3, position=pd, alpha = .5) + 
  scale_x_discrete(labels=labels, name="") + 
  geom_hline(yintercept =1, linetype='dashed', color='red', 14) +                                         theme(legend.position="top") + 
  scale_colour_d3(name="Field") + 
  ylab("MASE")

# Proportion of top 5 performers by discipline across all domains
proportions(xtabs( ~ discipline,top.5.MASE.t2))*100 #in total

# Proportion of top 5 performers by discipline and domain
proportions(xtabs( ~ domain+discipline,top.5.MASE.t2),"domain")*100 #by domain

# Performance of top 5 by prior forecasting experience and domain.
#### Fig. S11. in manuscript

top.5.MASE.t2  %>% 
  ggplot(aes(x=domain, y=MASE1_w2, colour=as.factor(previous_tournament.coded))) +  
  geom_point(size=3, position=pd, alpha = .5) + 
  scale_x_discrete(labels=labels, name="") + 
  geom_hline(yintercept =1, linetype='dashed', color='red', 14) +                                         theme(legend.position="top") + 
  scale_colour_d3(name="Prior Forecasting Experience") + 
  ylab("MASE")

# Proportion of top 5 who had previous tournament experience across all domains
proportions(xtabs( ~ previous_tournament.coded,top.5.MASE.t2))*100 #in total

# Proportion of top 5 who had previous tournament experience - academics only
proportions(xtabs( ~ previous_tournament.coded,academic_only%>% filter(!(phase == 1 & revised == 1))))*100 #baserate of prior experience to compare to top 5

# Proportion of top 5 who had previous tournament experience by domain
proportions(xtabs( ~ domain+previous_tournament.coded,top.5.MASE.t2),"domain")*100 #by domain


# Size of top ten teams by domain
academic_only %>% 
  filter(!(phase == 1 & revised == 1)) %>% 
  arrange(domain,MASE1_w2) %>%
  group_by(domain) %>% 
  dplyr::slice_head(n = 10) %>% dplyr::select(team_name,MASE1_w2,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise) %>%
  ggplot(aes(x = domain, y = team_size.coded)) +
  stat_summary(fun.data="mean_cl_boot",  position=pd)+theme_minimal(base_size = 14) +
  theme(legend.position="bottom") + 
  scale_x_discrete(labels=labels, name="") +
  labs(colour = "Approach",fill="Approach", x="",y="Size of Top 10 Teams (M +/- 95%CI)")
 
# Top 10 teams model complexity by domain.
academic_only %>% 
  filter(!(phase == 1 & revised == 1)) %>% 
  arrange(domain,MASE1_w2) %>% 
  group_by(domain) %>% 
  dplyr::slice_head(n = 10) %>% dplyr::select(team_name,MASE1_w1,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise) %>%
  ggplot(aes(x = domain, y = Method.complex)) +
  stat_summary(fun.data="mean_cl_boot", position=pd)+theme_minimal(base_size = 14) +
  theme(legend.position="bottom") + 
  scale_x_discrete(labels=labels, name="") +
  labs(colour = "Approach",fill="Approach", x="",y="Model complexity (M +/- 95%CI)")

# Top 5 teams model complexity by domain.
academic_only %>% 
  filter(!(phase == 1 & revised == 1)) %>% 
  arrange(domain,MASE1_w2) %>% 
  group_by(domain) %>% 
  dplyr::slice_head(n = 5) %>% dplyr::select(team_name,MASE1_w1,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise) %>%
  ggplot(aes(x = domain, y = Method.complex)) +
  stat_summary(fun.data="mean_cl_boot",  position=pd) + 
  theme_minimal(base_size = 14) +
  theme(legend.position="bottom") + 
  scale_x_discrete(labels=labels, name="") +
  labs(colour = "Approach",fill="Approach", x="",y="Model complexity (M +/- 95%CI)") #same as for top 10

# %of females per top 10 winning teams by domain.
academic_only %>% 
  filter(!(phase == 1 & revised == 1)) %>% 
  arrange(domain,MASE1_w2) %>% 
  group_by(domain) %>% 
  dplyr::slice_head(n = 10) %>% dplyr::select(team_name,MASE1_w2,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise,team_gender,team_education,team_Age,non_US )%>%
  ggplot(aes(x = domain, y = team_gender)) +
  stat_summary(fun.data="mean_cl_boot",  position=pd)+theme_minimal(base_size = 14) +
  theme(legend.position="bottom") + 
  scale_x_discrete(labels=labels, name="")+
  labs(colour = "Approach",fill="Approach", x="",y="% Female per Team (M +/- 95%CI)")


# % of non-Phds per team.
academic_only %>% 
  filter(!(phase == 1 & revised == 1)) %>% 
  arrange(domain,MASE1_w2) %>% 
  group_by(domain) %>% 
  dplyr::slice_head(n = 10) %>% dplyr::select(team_name,MASE1_w2,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise,team_gender,team_education,team_Age,non_US )%>%
  ggplot(aes(x = domain, y = team_education)) +
  stat_summary(fun.data="mean_cl_boot", position=pd)+theme_minimal(base_size = 14) +
  theme(legend.position="bottom") +scale_x_discrete(labels=labels, name="") +
  labs(colour = "Approach",fill="Approach", x="",y="% Non_PHD per Team (M +/- 95%CI)")

# Average team age for top 10 teams by domain.
academic_only %>% 
  filter(!(phase == 1 & revised == 1)) %>% 
  arrange(domain,MASE1_w2) %>% 
  group_by(domain) %>% 
  dplyr::slice_head(n = 10) %>%   dplyr::select(team_name,MASE1_w2,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise,team_gender,team_education,team_Age,non_US )%>%
  ggplot(aes(x = domain, y = team_Age)) +
  stat_summary(fun.data="mean_cl_boot",  position=pd)+theme_minimal(base_size = 14) +
  theme(legend.position="bottom") + 
  scale_x_discrete(labels=labels, name="") +
  labs(colour = "Approach",fill="Approach", x="",y="% Average Team Age (M +/- 95%CI)")

# % of non us individuals on top 10 teams by domain.
academic_only %>% 
  filter(!(phase == 1 & revised == 1)) %>% 
  arrange(domain,MASE1_w2) %>%
  group_by(domain) %>% 
  dplyr::slice_head(n = 10) %>% dplyr::select(team_name,MASE1_w2,domain,team_size.coded,discipline,previous_tournament.coded,Method.code,model,theory,numpred,parameters,Method.complex,team_expertise,team_gender,team_education,team_Age,non_US) %>%
  ggplot(aes(x = domain, y = non_US)) +
  stat_summary(fun.data="mean_cl_boot",  position=pd)+theme_minimal(base_size = 14) +
  theme(legend.position="bottom") + 
  scale_x_discrete(labels=labels, name="")+
  labs(colour = "Approach",fill="Approach", x="",y="% Non-US per Team (M +/- 95%CI)")

## Comparison by method among academics

# Percentage of academics who performed above, below, or just as well as naive rwf by method type across all domains.
proportions(xtabs( ~ compare_to_naive_rwf_MASE_w2+Method.code,academic_only %>% 
                     filter(!(phase == 1 & revised == 1))),"Method.code")*100 #

# Chi square test of the proportions above while also dropping equal to.
chisq.test(xtabs( ~ compare_to_naive_rwf_MASE_w2+Method.code,subset(academic_only%>% filter(!(phase == 1 & revised == 1)), compare_to_naive_rwf_MASE_w2!="Equal to Naive rwf"))) #exclude equal as it is negligible and screws up calculation

# Percentage of academics who performed above, below, or just as well as naive linear by method type across all domains
proportions(xtabs( ~ compare_to_naive_linear_MASE_w2+Method.code,academic_only%>% filter(!(phase == 1 & revised == 1))),"Method.code")*100 #

# Chi square test of proportions above.
chisq.test(xtabs( ~ compare_to_naive_linear_MASE_w2+Method.code,academic_only%>% filter(!(phase == 1 & revised == 1))))


#examine if top 5 in T1 are also among top 5 in t2
top.5.MASE.t1$phase<-"T1"
top.5.MASE.t1$MASE<-top.5.MASE.t1$MASE1_w1
top.5.MASE.t2$phase<-"T2"
top.5.MASE.t2$MASE<-top.5.MASE.t2$MASE1_w2





top.5.MASE<-rbind(top.5.MASE.t1,top.5.MASE.t2) 
top.5.MASE%>% dplyr::select(team_name,MASE,domain,phase,model,revised) %>% group_by(domain) %>% count(team_name)
#only in five out of 12 domains one top team from the first tournament appeared among the top five teams of a given domain in the second tournament: 
#Compassionate Values for Explicit African American bias; fearfulastra for Explicit Gender-Career bias; FMTeam for Implicit Asian American bias; AbCdEfG for Ideological Support of Democrats; A Woman Scientist for Negative Sentiment; NYHC for political polarization. The remaining top five teams were unique across tournaments. 

#examine consistency across domains in each tournament
top5.repeats.t1<-top.5.MASE.t1%>% dplyr::select(team_name,MASE,domain,n_domains) %>% group_by(team_name) %>% count(team_name) %>% arrange(desc(n))
psych::describe(top5.repeats.t1)#14 appear more than once; but M is small = 1.62
top5.repeats.t1.perc<-top5.repeats.t1 %>% left_join(top.5.MASE.t1 %>% dplyr::select(team_name,n_domains) )
top5.repeats.t1.perc$perc<-top5.repeats.t1.perc$n/top5.repeats.t1.perc$n_domains*100
print(top5.repeats.t1.perc) # one team among those who were among the top five in more than 2 domains had a reasonably small number of domains they made predictions about (6), such that in 4 out of 6 = 67% they were in the top five. For the rest, the number of domains they were in the top five were below half of those they made predictions for.   

top5.repeats.t2<-top.5.MASE.t2%>% dplyr::select(team_name,MASE,domain,n_domains) %>% group_by(team_name) %>% count(team_name) %>% arrange(desc(n))
psych::describe(top5.repeats.t2)#17 appear more than once; but M is small = 1.67
top5.repeats.t2.perc<-top5.repeats.t2 %>% left_join(top.5.MASE.t2 %>% dplyr::select(team_name,n_domains) )
top5.repeats.t2.perc$perc<-top5.repeats.t2.perc$n/top5.repeats.t2.perc$n_domains*100
print(top5.repeats.t2.perc) # one team among those who were among the top five in more than 2 domains had a reasonably small number of domains they made predictions about (9), such that in 5 out of 9 = 55.56% they were in the top five. For the rest, the number of domains they were in the top five were at/below half of those they made predictions for.   

```

# Visualize historical results

```{r INSPECT HISTORICAL TRENDS AND CALCULATE COMPLEXITY}
#THE SECTION BELOW IS CHIEFLY FOR INFORMATION ABOUT THE HISTORICAL TRENDS, AND VISUALLY INSPECTING VARIABILITY IN TRENDS, AS OUTLINED IN ONE SENTENCE IN THE DISCUSSION OF THE PAPER.

# Read in historical data
historical <- read.csv("historical_data.csv")
historical_tsbl <- as_tsibble(historical, index = Month)

# Actual change based on survey results for each domain.
historical_tsbl %>% 
  pivot_longer(negaffect:polar,names_to="Domain",values_to="Score") %>% 
  ggplot(aes(x = Month, y = Score, colour = Domain))+
  geom_smooth(aes(x = Month, y = Score, colour = Domain),method = "loess") +  
    facet_wrap(~Domain, scales = "free", nrow = 3, labeller=labeller(Domain=labels))+
  theme_minimal(base_size = 14) +
   theme(legend.position="none") +
  labs(x="Months (< 0 = before May 2020)",y="Estimate") 

# Historical long

## Convert data.frame into a tibble long format.
hist_long <- as_tibble(historical_tsbl) %>%
  pivot_longer(negaffect:polar,names_to="domain", values_to="Score")
#examine SD of all domains for historical data

#THIS SECTION BELOW COMPUTES MARKERS OF COMPLEXITY FOR THE TOURNAMENT, INCLUDING SD, MAD, AND AN SUPPLEMENTARY METRIC OF PERMUTATION ENTROPY

# Compute sd, mad, and entropy bt domain
hist_var_w1 <- as_tibble(historical_tsbl) %>% 
    subset(Month < 0) %>% 
    pivot_longer(negaffect:polar,names_to="domain",values_to="Score") %>%
    dplyr::select(domain,Score) %>% 
    group_by(domain) %>% 
    summarise(sd_hist_w1 = sd(Score), mad_hist_w1 = mad(Score), perp_entropy_hist_w1=permutation_entropy(Score))


tournament1_var <- 
  as_tibble(historical_tsbl) %>% 
  subset(Month > 0) %>% 
  pivot_longer(negaffect:polar,names_to="domain",values_to="Score") %>%
  dplyr::select(domain,Score) %>% 
  na.omit %>% 
  group_by(domain) %>% 
  summarise(sd_w1 = sd(Score), mad_w1 = mad(Score), perp_entropy_w1=permutation_entropy(Score))

hist_var_w2 <- as_tibble(historical_tsbl) %>% 
  subset(Month < 7) %>% 
  pivot_longer(negaffect:polar,names_to="domain",values_to="Score") %>%
  dplyr::select(domain,Score) %>% 
  group_by(domain) %>% 
  summarise(sd_hist_w2 = sd(Score), mad_hist_w2 = mad(Score), perp_entropy_hist_w2=permutation_entropy(Score))

tournament2_var <- as_tibble(historical_tsbl) %>% 
  subset(Month > 6) %>% 
  pivot_longer(negaffect:polar,names_to="domain",values_to="Score") %>%
  dplyr::select(domain,Score) %>% 
  na.omit%>%group_by(domain) %>% 
  summarise(sd_w2 = sd(Score), mad_w2 = mad(Score), perp_entropy_w2=permutation_entropy(Score))

complexity <- hist_var_w1 %>% 
  left_join(tournament1_var) %>%
  left_join(hist_var_w2) %>%
  left_join(tournament2_var)

```


# Visualizations

## Visualizations of predictions across domains

```{r PHASE 1 prep and simple visualizations of trends}

#do by method (among experts now)
#reorder levels of the domains
dat_long$domain <- factor(dat_long$domain,      # Reordering group factor levels
                         levels = c("egend","easian","eafric",
                                    "igend","iasian","iafric",
                                    "posaffect","negaffect","lifesat",
                                    "polar","ideoldem","ideolrep"))

#get ground truth markers (subset)
dat_long$Month0<-dat_long$Month-1

objective<-as.data.frame(subset(dat_long,phase == 1 & !is.na(Method.code)& Method.code=="Ground Truth"))

#get subset for supplementary analyses, not in the paper(!), focusing on value.dif column i -  absolute percent deviation for each predicted Month

dat_long_phase1<-dat_long %>%subset(phase == 1 & Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw")
dat_long_phase1$Method.code <- relevel(factor(dat_long_phase1$Method.code), "Lay People") #use lay people as a reference group

#updates for the coding of categories
phase1$Method.code <- relevel(factor(phase1$Method.code), "Lay People") #use lay people as a reference group
phase1_exp$updated<-ifelse(phase1_exp$revised==1,"update","no update")
phase1$compare_to_naive_rwf_MASE.update<-ifelse(phase1$compare_to_naive_rwf_MASE!="Equal to Naive rwf",phase1$compare_to_naive_rwf_MASE,ifelse(phase1$compare_to_naive_rwf_MASE=="Equal to Naive rwf","Below Naive rwf",NA))
phase1_exp$teamS<-as.factor(ifelse(phase1_exp$team_size.coded>=6,3,ifelse(phase1_exp$team_size.coded<6&phase1_exp$team_size.coded>1,2,ifelse(phase1_exp$team_size.coded==1,1,NA))))
phase1_exp$is_multidisciplinary<-ifelse(phase1_exp$discipline=="Multi-disciplinary",1,0)
phase1_exp$objectivexpert<-ifelse(phase1_exp$pub==1,"Expert",ifelse(phase1_exp$pub==2,"Non Expert",NA))

#add historical variability data (as extra variable)
phase1_exp<-complexity %>% left_join(phase1_exp)

#count how many domains per person
phase1_exp<-phase1_exp %>%group_by(team_name) %>% 
 mutate(n_domains = n())

#Supplementary analyses NOT in the paper: For models evaluating accuracy of individual time points, we will use forecasting type (purely theoretical, purely data-driven and hybrid models), forecasting domain and time points as predictors, with absolute percent deviation scores nested within teams. 

dat_long_phase1$teamS<-as.factor(ifelse(dat_long_phase1$team_size.coded>=6,3,ifelse(dat_long_phase1$team_size.coded<6&dat_long_phase1$team_size.coded>1,2,ifelse(dat_long_phase1$team_size.coded==1,1,NA))))
dat_long_phase1$is_multidisciplinary<-ifelse(dat_long_phase1$discipline=="Multi-disciplinary",1,0)
dat_long_phase1$objectivexpert<-ifelse(dat_long_phase1$pub==1,"Expert",ifelse(dat_long_phase1$pub==2,"Non Expert",NA))

###############################################################
#graph individual predictions (supplementary, NOT in the paper)
#BEGINNING
###############################################################


dat_long %>% subset(phase == 1 & !is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw") %>% 
   ggplot(aes(x = Month0, y = value, colour = Method.code, fill=Method.code))+
  geom_smooth(aes(x = Month0, y = value, colour = Method.code, fill=Method.code),method = "loess") +  
  facet_wrap(~domain, scales = "free", nrow = 3, labeller=labeller(domain=labels))+theme_minimal(base_size = 14) +
  geom_smooth(data=objective,se=F) + #here we add the ground truth markers without confidence band
   theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+ 
  labs(colour = "Sample",fill="Sample", x="Months (from May 2021)",y="Estimate (M +/- 95%CI)")
##without any benchmarks
dat_long$GT[dat_long$Method.code!="Ground Truth"]<-"Forecasting Estimate"
dat_long$GT[dat_long$Method.code=="Ground Truth"]<-"Ground Truth"
objective$GT<-objective$Method.code
dat_long %>% subset(phase == 1 & !is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" &Method.code!="Lay People") %>% 
   ggplot(aes(x = Month0, y = value, colour = GT, fill=GT))+
  geom_smooth(aes(x = Month0, y = value, colour = GT, fill=GT),method = "loess") +  
  facet_wrap(~domain, scales = "free", nrow = 3, labeller=labeller(domain=labels))+theme_minimal() +
  geom_smooth(data=objective,se=F) + #here we add the ground truth markers without confidence band
   theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+ 
  labs(colour = "",fill="", x="Months (from May 2021)",y="Estimate (M +/- 95%CI)")

hist_long$Domain<-hist_long$domain
hist_long$GT<-"Ground Truth"
hist_long$Month0<-hist_long$Month-1
hist_long$value<-hist_long$Score

dat_longX<-dat_long %>% subset(phase == 1 & !is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" &Method.code!="Lay People") #to get extra scores for indiv point visualization.
dat_longX$GT<-"Forecasts from\nIndiv. Teams"

dat_long$GT[dat_long$Method.code!="Ground Truth"]<-"Forecasting Estimate"

dat_long %>% subset(phase == 1 & !is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" &Method.code!="Lay People") %>% 
   ggplot(aes(x = as.numeric(Month0), y = value, colour = GT, fill=GT))+geom_line(data=dat_longX,alpha=.3,aes(group=team_name), na.rm=TRUE)+  geom_point(data = dat_longX,alpha=.1,aes(group=team_name)) +
  geom_smooth(aes(x = Month0, y = value, colour = GT, fill=GT),method = "loess") +  
  facet_wrap(~domain, scales = "free", nrow = 6, labeller=labeller(domain=labels))+theme_minimal() +
  geom_smooth(data=hist_long,se=F) + #here we add the ground truth markers without confidence band
   theme(legend.position="bottom") +scale_color_npg()+scale_fill_npg()+ 
  labs(colour = "",fill="", x="Months (< 0 = before May 2020 Tournament)",y="Estimate (M +/- 95%CI)")

dat_long %>% subset(phase == 1 & !is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" &Method.code!="Lay People") %>% 
   ggplot(aes(x = as.numeric(Month0), y = value, colour = GT, fill=GT))+geom_line(data=dat_longX,alpha=.3,aes(group=team_name), na.rm=TRUE)+  geom_point(data = dat_longX,alpha=.1,aes(group=team_name)) +
  geom_smooth(aes(x = Month0, y = value, colour = GT, fill=GT),method = "loess") +  
  facet_wrap(~domain, scales = "free", nrow = 6, labeller=labeller(domain=labels))+theme_minimal() +
  geom_smooth(data=objective,se=F) + #here we add the ground truth markers without confidence band
   theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+ 
  labs(colour = "",fill="", x="Months (< 0 = before May 2020 Tournament)",y="Estimate (M +/- 95%CI)")

###############################################################
#graph individual predictions (supplementary, NOT in the paper)
#END
###############################################################

###############################################################
#graph individual predictions and ground truth markers - FIGURE 1 IN THE SUPPLEMENT in the PAPER)
#BEGINNING
##this one includes creating subsets, historical data subsets, and designing sub-plots of various caliber for the paper, and for presentations and putting the subplots together
###############################################################

#combine with the phase 2 data.
dat_long_phase2X<-dat_long %>%filter(!(phase == 1 & revised == 1)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" & Month %in% c(7,8,9,10,11,12))

dat_long$GT[dat_long$Method.code!="Ground Truth"]<-"Aggregate Estimate\n(lowess)"
objective$value
dat_long$phaseF[dat_long$phase==1]<-"First Tournament\n(May 2020)"
dat_long$phaseF[dat_long$phase==2]<-"Follow-up Tournament\n(Nov 2020)" 

objective$phaseF<-"Ground Truth"
dat_longX<-dat_long %>% subset(!is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" &Method.code!="Lay People"& Month %in% c(1:12)) 
                               #to get extra scores for indiv point visualization 
dat_longX<-dat_longX %>%subset(!(domain=="lifesat" & value <6)) #cut off scores below 5 for life satisfaction for visualization of trends

dat_longX$GT<-"Forecasts from\nIndiv. Teams"

dat_long$date[dat_long$Month==1] <- "05-2020"
dat_long$date[dat_long$Month==2] <- "06-2020"
dat_long$date[dat_long$Month==3] <- "07-2020"
dat_long$date[dat_long$Month==4] <- "08-2020"
dat_long$date[dat_long$Month==5] <- "09-2020"
dat_long$date[dat_long$Month==6] <- "10-2020"
dat_long$date[dat_long$Month==7] <- "11-2020"
dat_long$date[dat_long$Month==8] <- "12-2020"
dat_long$date[dat_long$Month==9] <- "01-2021"
dat_long$date[dat_long$Month==10] <- "02-2021"
dat_long$date[dat_long$Month==11] <- "03-2021"
dat_long$date[dat_long$Month==12] <- "04-2021"

dat_long$date<-my(dat_long$date)
objective$date[objective$Month==1] <- "05-2020"
objective$date[objective$Month==2] <- "06-2020"
objective$date[objective$Month==3] <- "07-2020"
objective$date[objective$Month==4] <- "08-2020"
objective$date[objective$Month==5] <- "09-2020"
objective$date[objective$Month==6] <- "10-2020"
objective$date[objective$Month==7] <- "11-2020"
objective$date[objective$Month==8] <- "12-2020"
objective$date[objective$Month==9] <- "01-2021"
objective$date[objective$Month==10] <- "02-2021"
objective$date[objective$Month==11] <- "03-2021"
objective$date[objective$Month==12] <- "04-2021"
objective$date<-my(objective$date)


dat_long %>% subset(!is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" &Method.code!="Lay People"& Month %in% c(1:12)) %>% 
   ggplot(aes(x = Month0, y = value, colour = phaseF, fill=phaseF))+geom_line(data=dat_longX,alpha=.09,aes(x = Month0, group=team_name), na.rm=TRUE)+  #geom_point(data = dat_longX,alpha=.1,aes(x = Month0, group=team_name)) +
  geom_smooth(aes(x = Month0, y = value, colour = phaseF, fill=phaseF),method = "loess") +  
  facet_wrap(~domain, scales = "free", nrow = 4, labeller=labeller(domain=labels))+theme_minimal() +
  geom_line(data=objective,alpha=.8,aes(x = Month0, group=team_name), na.rm=TRUE)+  geom_point(data = objective,alpha=.9,aes(x = Month0)) + #here we add the ground truth markers without confidence band
   theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+ xlim(0,12)+ scale_x_continuous(breaks=c(0:11), labels =c("May","Jun","Jul","Aug","Sep","Oct","Nov","Dec","Jan","Feb","Mar","Apr"))+  labs(colour = "",fill="", x="Months",y="Forecasted & Observed Change")+theme(axis.text.x = element_text(angle=45, vjust=.5, hjust=1, size=rel(0.5)))

hist_long$phaseF<-hist_long$GT

#reorder levels of the domains
hist_long$domain <- factor(hist_long$domain,      # Reordering group factor levels
                         levels = c("egend","easian","eafric",
                                    "igend","iasian","iafric",
                                    "posaffect","negaffect","lifesat",
                                    "polar","ideoldem","ideolrep"))

labels<-c(
  eafric = "Exp. Bias Vs. Afr.-Am\nhigher=stereo-consistent\n(-3 to +3)",
  easian = "Exp. Bias Vs. Asian.-Am\nhigher=stereo-consistent\n(-3 to +3)",
  egend = "Exp. Bias Vs. Women-Career\nhigher=stereo-consistent\n(-3 to +3)",
  iafric = "Imp. Bias Vs. Afr.-Am.\nhigher=stereo-consistent\n(IAT D score)",
  iasian = "Imp. Bias Vs. Asian.-Am.\nhigher=stereo-consistent\n(IAT D score)",
  ideoldem = "Democratic Support\n(% Population)",
  ideolrep ="Republican Support\n(% Population)",
  igend = "Imp. Bias Vs. Women-Career\nhigher=stereo-consistent\n(IAT D score)",
  lifesat = "Life Satisfaction\nCantril ladder\n(0-10 scale)",
  negaffect = "Negative Affect\nstandardized Vs. historical M/SD\n(z-score)",
  polar = "Polit. Polarization\n% of Rep. Vs. Dem. approvals\n(absolute difference score) ",
  posaffect = "Positive Affect\n(z-score)")

objective.t<-subset(hist_long,Month>0)
hist.t<-subset(hist_long,Month %in% c(-2:-1))
hist.t$phaseF<-"Historical"

# with 3 historical months before the tournament
hist_long%>% subset(Month %in% c(-2:12))%>%
   ggplot(aes(x = Month, y = value, colour = phaseF, fill=phaseF))+  
  theme_pubclean()+
  geom_line(data=objective.t,alpha=.8,aes(x = Month), na.rm=TRUE)+  geom_point(data=objective.t,alpha=.9,aes(x = Month)) +geom_line(data=hist.t,alpha=.8,aes(x = Month), na.rm=TRUE)+  geom_point(data=hist.t,alpha=.9,aes(x = Month))+   theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+ scale_x_continuous(breaks=c(-2:12), labels =c("Feb","","","May","","","Aug","","","Nov","","","Feb","",""))+  labs(colour = "",fill="", x="",y="Forecasted & Observed Change")+facet_wrap(~domain, scales = "free_y", nrow = 4, labeller=labeller(domain=labels))+ geom_line(data=dat_longX,alpha=.09,aes(x = Month, group=team_name), na.rm=TRUE)+  #geom_point(data = dat_longX,alpha=.1,aes(x = Month0, group=team_name)) +
  theme(axis.text.x = element_text(angle=45, vjust=.5, hjust=1, size=rel(0.8)))+geom_smooth(data=dat_long %>% subset(!is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" &Method.code!="Lay People"& Month %in% c(-2:12)), aes(x = Month, y = value, colour = phaseF, fill=phaseF),method = "loess") 

# select just negative affect 
plot.negaffect<-hist_long%>% subset(Month %in% c(-2:12) & domain %in% c("negaffect"))%>%
   ggplot(aes(x = Month, y = value, colour = phaseF, fill=phaseF))+  
  theme_pubclean()+
  geom_line(data=subset(objective.t,domain %in% c("negaffect")),alpha=.8,aes(x = Month), na.rm=TRUE)+  geom_point(data=subset(objective.t, domain %in% c("negaffect")),alpha=.9,aes(x = Month)) +geom_line(data=subset(hist.t,domain %in% c("negaffect")),alpha=.8,aes(x = Month), na.rm=TRUE)+  geom_point(data=subset(hist.t,domain %in% c("negaffect")),alpha=.9,aes(x = Month))+   theme(legend.position="bottom") +  theme(legend.position="bottom", legend.text = element_text(size=7)) +scale_color_d3()+scale_fill_d3()+ scale_x_continuous(breaks=c(-2:12), labels =c("Feb","","","May","","","Aug","","","Nov","","","Feb","",""))+  labs(colour = "",fill="", x="",y="z-score", title="Negative Affect", subtitle = "Standartized against historical M/SD")+ geom_line(data=subset(dat_longX, domain %in% c("negaffect")),alpha=.13,aes(x = Month, group=team_name), na.rm=TRUE)+  #geom_point(data = dat_longX,alpha=.1,aes(x = Month0, group=team_name)) +
  theme(axis.text.x = element_text(angle=45, vjust=.5, hjust=1, size=rel(0.8)))+geom_smooth(data=dat_long %>% subset(!is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" &Method.code!="Lay People"& Month %in% c(-2:12) & domain %in% c("negaffect")), aes(x = Month, y = value, colour = phaseF, fill=phaseF),method = "loess") +theme(plot.title = element_text(hjust = 0.5),plot.subtitle = element_text(hjust = 0.5))

# select pos affect and life satisfaction

plot.LS.and.posaffect<-hist_long%>% subset(Month %in% c(-2:12)& domain %in% c("posaffect", "lifesat"))%>%
   ggplot(aes(x = Month, y = value, colour = phaseF, fill=phaseF))+  
  theme_pubclean()+
 geom_line(data=subset(objective.t,domain %in% c("posaffect", "lifesat")),alpha=.8,aes(x = Month), na.rm=TRUE)+  geom_point(data=subset(objective.t, domain %in% c("posaffect", "lifesat")),alpha=.9,aes(x = Month)) +geom_line(data=subset(hist.t,domain %in% c("posaffect", "lifesat")),alpha=.8,aes(x = Month), na.rm=TRUE)+  geom_point(data=subset(hist.t,domain %in% c("posaffect", "lifesat")),alpha=.9,aes(x = Month))+
  theme(legend.position="none") +scale_color_d3()+scale_fill_d3()+ scale_x_continuous(breaks=c(-2:12), labels =c("Feb","","","May","","","Aug","","","Nov","","","Feb","",""))+  labs(colour = "",fill="", x="",y="")+facet_wrap(~domain, scales = "free_y", nrow = 4, labeller=labeller(domain=labels))+ geom_line(data=subset(dat_longX, domain %in% c("posaffect", "lifesat")) ,alpha=.09,aes(x = Month, group=team_name), na.rm=TRUE)+  #geom_point(data = dat_longX,alpha=.1,aes(x = Month0, group=team_name)) +
  theme(axis.text.x = element_text(angle=45, vjust=.5, hjust=1, size=rel(0.8)))+geom_smooth(data=dat_long  %>% subset(!is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" &Method.code!="Lay People"& Month %in% c(-2:12)& domain %in% c("posaffect", "lifesat")), aes(x = Month, y = value, colour = phaseF, fill=phaseF),method = "loess") 

plot.wb<-ggarrange(plot.negaffect,plot.LS.and.posaffect,  ncol=2, nrow=1,widths=c(2,1))
#graph for slides

##graph for paper
# select just negative affect 
plot.negaffectX<-hist_long%>% subset(Month %in% c(-2:12) & domain %in% c("negaffect"))%>%
   ggplot(aes(x = Month, y = value, colour = phaseF, fill=phaseF))+  
  theme_pubclean()+
  geom_line(data=subset(objective.t,domain %in% c("negaffect")),alpha=.8,aes(x = Month), na.rm=TRUE)+  geom_point(data=subset(objective.t, domain %in% c("negaffect")),alpha=.9,aes(x = Month)) +geom_line(data=subset(hist.t,domain %in% c("negaffect")),alpha=.8,aes(x = Month), na.rm=TRUE)+  geom_point(data=subset(hist.t,domain %in% c("negaffect")),alpha=.9,aes(x = Month))+
  theme(legend.position="none") +scale_color_d3()+scale_fill_d3()+ scale_x_continuous(breaks=c(-2:12), labels =c("Feb","","","May","","","Aug","","","Nov","","","Feb","",""))+  labs(colour = "",fill="", x="",y="z-score", title="Negative Affect", subtitle = "Standardized against historical M/SD")+ geom_line(data=subset(dat_longX, domain %in% c("negaffect")),alpha=.13,aes(x = Month, group=team_name), na.rm=TRUE)+  #geom_point(data = dat_longX,alpha=.1,aes(x = Month0, group=team_name)) +
  theme(axis.text.x = element_text(angle=45, vjust=.5, hjust=1, size=rel(0.8)))+geom_smooth(data=dat_long %>% subset(!is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" &Method.code!="Lay People"& Month %in% c(-2:12) & domain %in% c("negaffect")), aes(x = Month, y = value, colour = phaseF, fill=phaseF),method = "loess") +theme(plot.title = element_text(hjust = 0.5),plot.subtitle = element_text(hjust = 0.5))

plot.wbX<-ggarrange(plot.negaffectX,plot.LS.and.posaffect,  ncol=2, nrow=1,widths=c(1.8,1))

#biases and politics
plot.all.but.WB<-hist_long%>% subset(Month %in% c(-2:12)& domain %in% c("egend","easian","eafric",
                                    "igend","iasian","iafric","polar","ideoldem","ideolrep"))%>%
   ggplot(aes(x = Month, y = value, colour = phaseF, fill=phaseF))+  
  theme_pubclean()+
  geom_line(data=subset(objective.t,domain %in% c("egend","easian","eafric",
                                    "igend","iasian","iafric","polar","ideoldem","ideolrep")),alpha=.8,aes(x = Month), na.rm=TRUE)+  geom_point(data=subset(objective.t, domain %in% c("egend","easian","eafric",
                                    "igend","iasian","iafric","polar","ideoldem","ideolrep")),alpha=.9,aes(x = Month)) +geom_line(data=subset(hist.t,domain %in% c("egend","easian","eafric",
                                    "igend","iasian","iafric","polar","ideoldem","ideolrep")),alpha=.8,aes(x = Month), na.rm=TRUE)+  geom_point(data=subset(hist.t,domain %in% c("egend","easian","eafric",
                                    "igend","iasian","iafric","polar","ideoldem","ideolrep")),alpha=.9,aes(x = Month))+
  theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+ scale_x_continuous(breaks=c(-2:12), labels =c("Feb","","","May","","","Aug","","","Nov","","","Feb","",""))+  labs(colour = "",fill="", x="",y="")+facet_wrap(~domain, scales = "free_y", nrow = 4, labeller=labeller(domain=labels))+ geom_line(data=subset(dat_longX, domain %in% c("egend","easian","eafric",
                                    "igend","iasian","iafric","polar","ideoldem","ideolrep")) ,alpha=.09,aes(x = Month, group=team_name), na.rm=TRUE)+  #geom_point(data = dat_longX,alpha=.1,aes(x = Month0, group=team_name)) +
  theme(axis.text.x = element_text(angle=45, vjust=.5, hjust=1, size=rel(0.8)))+geom_smooth(data=dat_long  %>% subset(!is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" &Method.code!="Lay People"& Month %in% c(-2:12)& domain %in% c("egend","easian","eafric",
                                    "igend","iasian","iafric","polar","ideoldem","ideolrep")), aes(x = Month, y = value, colour = phaseF, fill=phaseF),method = "loess") 

#combine into megaplot
### Fig. S1. in the manuscript

plot.all<-ggarrange(plot.wbX,plot.all.but.WB,  ncol=1, nrow=2, heights = c(1.2,1.8),  common.legend = TRUE, legend="bottom")

plot.all
###############################################################
#graph individual predictions and ground truth markers - IN THE SUPPLEMENT in the PAPER
#END
###############################################################


```

## Visualizations of MASE versus benchmarks

```{r Phases 1 and 2 along with sims}
###############################################################
#graph individual predictions and ground truth markers - FIGURE 2 IN THE SUPPLEMENT in the PAPER, as well as one FIGURE in the MAIN TEXT)
#ALSO: analyses of scientists versus lay people in tournament 1
#BEGINNING
##this one includes creating model estimates for tournament 1 and tournament 2 for academics (and lay people in tournament 1 - we focus on linear mixed model estimates to account for interdependence in predictions), saving mean estimates and CIs, combining with benchmarks, and designing plots of various caliber for the paper
###############################################################
pd <- position_dodge(0.7) # move them .07 to the left and right

##by method for phase 1
###inspect data for distribution properties

hist(log(phase1$MASE1_w1)) #possibly do it on logs?
describe(phase1$MASE1_w1)

#analyses of phase 1  - MASE overall, without an interaction
model.phase1.together.nodomain.interac<-  lmer(log(MASE1_w1)~isExpert.factor+domain+(1|ResponseId), data=phase1)
car::Anova(model.phase1.together.nodomain.interac,type="III", test.statistic="F") #sig main effect, but only if we don't include interaction. 
summ(model.phase1.together.nodomain.interac, digits = 5)
partR2(model.phase1.together.nodomain.interac,partvars=c("isExpert.factor"), nboot=100)
#Pseudo-RÂ² (fixed effects) = 0.0132


model.phase1.together<-  lmer(log(MASE1_w1)~domain*isExpert.factor+(1|ResponseId), data=phase1)
car::Anova(model.phase1.together,type="III", test.statistic="F") #sig interaction!
data.phase1.MASE.together<-as.data.frame(emmeans(model.phase1.together, pairwise~domain*isExpert.factor, adjust = "none", type = "response")$emmeans) #backtransformed to the original scale
summ(model.phase1.together, digits = 8)
#Pseudo-RÂ² (fixed effects) = 0.28053205
partR2(model.phase1.together,partvars=c("isExpert.factor","isExpert.factor:domain"), nboot=100)

#test the difference between experts and lay people
model.layVSsci.phase1<-  lmer(log(MASE1_w1)~domain*isExpert.factor+(1|ResponseId), data=phase1)
car::Anova(model.layVSsci.phase1,type="III",test.statistic="F") #sig interaction!
data.phase1.MASE.explaycomp<-as.data.frame(emmeans(model.layVSsci.phase1,pairwise ~isExpert.factor|domain,  type="response")$contrasts) #get the estimates in a dataframe

#get FDR correction across all pairwise tests
data.phase1.MASE.explaycomp$Hochberg <-p.adjust(data.phase1.MASE.explaycomp$p.value,
               method = "hochberg")
data.phase1.MASE.explaycomp


eff_size(emmeans(model.layVSsci.phase1,pairwise ~isExpert.factor|domain),sigma = sigma(model.layVSsci.phase1), edf =df.residual(model.layVSsci.phase1) ) #using the smallest DF among the three

#test Bayesian version of full model and estimated simple effects.
phase1$MASE1_w1_log<-log(phase1$MASE1_w1)
stan_model <- stan_lmer(MASE1_w1_log ~ domain*isExpert.factor + (1|ResponseId), data = phase1,prior = cauchy(0,c(0.707,0.707,0.5),          # as per Rouder et al., 2012, weakly informative
prior_intercept = student_t(3,0,10),prior_aux = exponential(.1),prior_covariance = decov(1,1,1,1))   
                        
em_expert_simple <- emmeans(stan_model, pairwise~isExpert.factor | domain)

bayesfactor_parameters(em_expert_simple, prior = stan_model)

#phase 2
dat_phase2<-academic_only %>%filter(!(phase == 1 & revised == 1)) #just academics, omitting original (non-revised phase 1)
model.phase2.together<-  lmer(log(MASE1_w2)~domain+(1|team_name), data=dat_phase2)
car::Anova(model.phase2.together,type="III") #sig interaction!
data.phase2.MASE.together<-as.data.frame(emmeans(model.phase2.together, pairwise~domain, adjust = "none", type = "response")$emmeans) #backtransformed to the original scale

data.phase1.MASE.together$Wave<-"First Tournament (May 2020)"
data.phase1.MASE.together$Type[data.phase1.MASE.together$isExpert.factor=="Academic"]<-"Scientists"
data.phase1.MASE.together$Type[data.phase1.MASE.together$isExpert.factor=="Prolific"]<-"Naive Crowd"


data.phase2.MASE.together$Wave<-"Second Tournament (Nov 2020)"
data.phase2.MASE.together$Type<-"Scientists"

#add simulation benchmarks & combine
means.compare.to.naive<-bind_rows(data.phase1.MASE.together,data.phase2.MASE.together,sim.w1,sim.w2)

#arrange in descending order based on MASE w1 of academics
means.compare.to.naive$domain<-factor(means.compare.to.naive$domain,levels=c("iafric","ideolrep","eafric",
  "negaffect", "lifesat","easian","ideoldem","iasian", "polar", "igend","posaffect","egend"))

#arrange in order of tournament factors
means.compare.to.naive$Wave<-factor(means.compare.to.naive$Wave,levels=c("First Tournament (May 2020)","Second Tournament (Nov 2020)"))

#arrange groups
means.compare.to.naive$Type<-factor(means.compare.to.naive$Type,levels=c("Scientists","Naive Crowd","Historic Mean","Random Walk","Linear Regression"))

#add var for Scientists vs. rest (to define colors)
means.compare.to.naive$Group[means.compare.to.naive$Type=="Scientists"]<-"Estimate"
means.compare.to.naive$Group[means.compare.to.naive$Type!="Scientists"]<-"Non Estimate"

labeling<-c(
  eafric = "Exp. Afr.-Am. Bias",
  easian = "Exp. Asian-Am. Bias",
  egend = "Exp. Gender Bias",
  iafric = "Imp. Afr.-Am. Bias",
  iasian = "Imp. Asian-Am. Bias",
  ideoldem = "Democrat. Support",
  ideolrep ="Republic. Support",
  igend = "Imp. Gender Bias",
  lifesat = "Life Satisfaction",
  negaffect = "Negative Affect",
  polar = "Polarization",
  posaffect = "Positive Affect")

#plot for the supplement
#### Fig. S2. in the manuscript
means.compare.to.naive %>%  
 ggplot(aes(x = response, y = domain, color = Type, shape=Type))+
 geom_pointrange(aes(xmin=lower.CL, xmax=upper.CL), position=pd)+  theme_minimal(base_size = 14)+geom_vline(xintercept =1, linetype='dotted', color='black',14)+
  geom_vline(xintercept =1.7665, linetype='dashed', color='blue',16)+theme(legend.position="bottom")+scale_color_jama()+  labs(x="Forecasting Error - MASE (M +/- 95%CI)",shape="",color="")+scale_y_discrete(labels=labeling, name="")+facet_grid(~Wave)

#create a main text version with top (lowest MASE) benchmark per domain instead of all three benchmarks
##first, get lowest benchmarks per domain
sim.w1.top<-sim.w1 %>% dplyr::select(domain,Mean) %>% summarise(response = min(Mean), Wave="First Tournament (May 2020)")
sim.w2.top<-sim.w2 %>% dplyr::select(domain,Mean) %>% summarise(response = min(Mean), Wave="Second Tournament (Nov 2020)")
#add simulation benchmarks & combine
means.compare.to.naive.top<-bind_rows(data.phase1.MASE.together,data.phase2.MASE.together,sim.w1.top,sim.w2.top)
#arrange in descending order based on MASE w1 of academics
means.compare.to.naive.top$domain<-factor(means.compare.to.naive.top$domain,levels=c("iafric","ideolrep","eafric",
  "negaffect", "lifesat","easian","ideoldem","iasian", "polar", "igend","posaffect","egend"))
#arrange in order of tournament factors
means.compare.to.naive.top$Wave<-factor(means.compare.to.naive.top$Wave,levels=c("First Tournament (May 2020)","Second Tournament (Nov 2020)"))
#arrange groups
means.compare.to.naive.top$Type[is.na(means.compare.to.naive.top$Type)==T]<-"Naive Statistic"
means.compare.to.naive.top$Type<-factor(means.compare.to.naive.top$Type,levels=c("Scientists","Naive Crowd","Naive Statistic"))
#add var for Scientists vs. rest (to define colors)
means.compare.to.naive.top$Group[means.compare.to.naive.top$Type=="Scientists"]<-"Estimate"
means.compare.to.naive.top$Group[means.compare.to.naive.top$Type!="Scientists"]<-"Non Estimate"

#plot for the main text
#### Figure 1. in the manuscript

means.compare.to.naive.top %>%  
 ggplot(aes(x = response, y = domain, color = Type, shape=Type))+
 geom_pointrange(aes(xmin=lower.CL, xmax=upper.CL), position=pd)+  theme_minimal(base_size = 14)+geom_vline(xintercept =1, linetype='dotted', color='black',14)+
  geom_vline(xintercept =1.7665, linetype='dashed', color='blue',16)+theme(legend.position="bottom")+scale_color_jama()+  labs(x="Forecasting Error - MASE (M +/- 95%CI)",shape="",color="")+scale_y_discrete(labels=labeling, name="")+facet_grid(~Wave)


#subplots for presentation (talks, etc)

#scientists
means.compare.to.naive %>%  subset(Type=="Scientists")%>%  
 ggplot(aes(x = response, y = domain, color = Type, shape=Type))+
 geom_pointrange(aes(xmin=lower.CL, xmax=upper.CL), position=pd)+  theme_minimal(base_size = 14)+geom_vline(xintercept =1, linetype='dotted', color='black',14)+
  geom_vline(xintercept =1.7665, linetype='dashed', color='blue',16)+theme(legend.position="bottom")+scale_color_jama()+  labs(x="Forecasting Error - MASE (M +/- 95%CI)",shape="",color="")+scale_y_discrete(labels=labeling, name="")+facet_grid(~Wave)

#scientists & lay crowd
means.compare.to.naive %>%  subset(Type=="Scientists"|Type=="Naive Crowd")%>%  
 ggplot(aes(x = response, y = domain, color = Type, shape=Type))+
 geom_pointrange(aes(xmin=lower.CL, xmax=upper.CL), position=pd)+  theme_minimal(base_size = 14)+geom_vline(xintercept =1, linetype='dotted', color='black',14)+
  geom_vline(xintercept =1.7665, linetype='dashed', color='blue',16)+theme(legend.position="bottom")+scale_color_jama()+  labs(x="Forecasting Error - MASE (M +/- 95%CI)",shape="",color="")+scale_y_discrete(labels=labeling, name="")+facet_grid(~Wave)
###############################################################
#graph individual predictions and ground truth markers - FIGURE 2 IN THE SUPPLEMENT in the PAPER, as well as one FIGURE in the MAIN TEXT)
#ALSO: analyses of scientists versus lay people in tournament 1
#END
```

#statistical tests of difference from benchmark
```{r}
########################################################################################
#THIS SECTION INCLUDES STATISTICAL TESTS AGAINST THE BENCHMARKS AND THEIR VISUALIZATION
########################################################################################
#to examine difference in inaccuracy from benchmark vs. domain estimates from scientists in the LME, we can do the following:

#1. create ratio of  benchmark inaccuracy to forecasting inaccuracy -  score above 1 means forecast is more accurate compared to the benchmark
#2 run an intercept model, to see if intercept is sig different from 1

##Tournament 1 - phase1_exp

phase1_exp_wbench<-phase1_exp %>% left_join(pivot_wider(sim.w1 %>% dplyr::select(domain,response, source),
                                            names_from="source",values_from="response"))

phase1_exp_wbench$MASE_ratio1<- phase1_exp_wbench$'Benchmark 1'/phase1_exp_wbench$MASE1_w1
phase1_exp_wbench$MASE_ratio2<- phase1_exp_wbench$'Benchmark 2'/phase1_exp_wbench$MASE1_w1
phase1_exp_wbench$MASE_ratio3<- phase1_exp_wbench$'Benchmark 3'/phase1_exp_wbench$MASE1_w1

phase1_exp_wbench$domain <- factor(phase1_exp_wbench$domain,      # Reordering group factor levels
                         levels = c("ideolrep","ideoldem","polar",
                                    "lifesat","negaffect","posaffect",
                                    "iafric","iasian","igend",
                                    "eafric","easian","egend" ))

#skewness test suggests that sqrt is the most reasonable transformation across the three metrics (esp. the first one) hence we will use it.

model.phase1.hist.mean.ratio<-  lmer(sqrt(MASE_ratio1)~domain+(1|team_name), data=phase1_exp_wbench)
Anova(model.phase1.hist.mean.ratio, test.statistic = "F")
phase1.hist.mean.ratio.means <- as.data.frame(emmeans(model.phase1.hist.mean.ratio,~domain, type="response"))
phase1.hist.mean.ratio.means$Estimate<-"Historical Mean"
phase1.hist.mean.ratio.means$Date<-"Tournament 1 (May 2020)"
phase1.hist.mean.ratio.means

#here are the results of the historical mean tests for Tournament 1

plot.t1.hist.mean<-plot(emmeans(model.phase1.hist.mean.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(labels=labeling, name="Historical Mean")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="Tournament 1 (May 2020)")

model.phase1.randwalk.ratio<-  lmer(sqrt(MASE_ratio2)~domain+(1|team_name), data=phase1_exp_wbench)
Anova(model.phase1.randwalk.ratio, test.statistic = "F")
phase1.randwalk.ratio.means<-as.data.frame(emmeans(model.phase1.randwalk.ratio,~domain, type="response"))
phase1.randwalk.ratio.means$Estimate<-"Random Walk"
phase1.randwalk.ratio.means$Date<-"Tournament 1 (May 2020)"
phase1.randwalk.ratio.means

#here are the results of the random walk tests for Tournament 1

plot.t1.randwalk<-plot(emmeans(model.phase1.randwalk.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(labels=labeling, name="Random Walk")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="")

model.phase1.linreg.ratio<-  lmer(sqrt(MASE_ratio3)~domain+(1|team_name), data=phase1_exp_wbench)
Anova(model.phase1.linreg.ratio, test.statistic = "F")
phase1.linreg.ratio.means<-as.data.frame(emmeans(model.phase1.linreg.ratio,~domain, type="response"))
phase1.linreg.ratio.means$Estimate<-"Linear Regression"
phase1.linreg.ratio.means$Date<-"Tournament 1 (May 2020)"
phase1.linreg.ratio.means

#here are the results of the linear regression tests for Tournament 1

plot.t1.linreg<-plot(emmeans(model.phase1.linreg.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(labels=labeling, name="Linear Regression")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="")


##examine if better than all 3 benchmarks

phase1_exp_wbench$phase1_exp_wbench_topBenchmark <- pmin(phase1_exp_wbench$'Benchmark 1',phase1_exp_wbench$'Benchmark 2',phase1_exp_wbench$'Benchmark 3')
phase1_exp_wbench$MASE_ratio1_topBenchmark<- phase1_exp_wbench$phase1_exp_wbench_topBenchmark/phase1_exp_wbench$MASE1_w1

#skewness test suggests that sqrt is the most reasonable transformation  hence we will use it.
model.phase1.topBenchmark.ratio<-  lmer(sqrt(MASE_ratio1_topBenchmark)~domain+(1|team_name), data=phase1_exp_wbench)
Anova(model.phase1.topBenchmark.ratio, test.statistic = "F")
emmeans(model.phase1.topBenchmark.ratio,~domain, type="response")


plot.t1.topBenchmark<-plot(emmeans(model.phase1.topBenchmark.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(labels=labeling, name="Top Naive Benchmark")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="Tournament 1 (May 2020)")


#Tournament 2

phase2_exp_wbench<-dat_phase2 %>% left_join(pivot_wider(sim.w2 %>% dplyr::select(domain,response, source),
                                            names_from="source",values_from="response"))

phase2_exp_wbench$domain <- factor(phase2_exp_wbench$domain,      # Reordering group factor levels
                         levels = c("ideolrep","ideoldem","polar",
                                    "lifesat","negaffect","posaffect",
                                    "iafric","iasian","igend",
                                    "eafric","easian","egend" ))

phase2_exp_wbench$MASE_ratio1<- phase2_exp_wbench$'Benchmark 1'/phase2_exp_wbench$MASE1_w2
phase2_exp_wbench$MASE_ratio2<- phase2_exp_wbench$'Benchmark 2'/phase2_exp_wbench$MASE1_w2
phase2_exp_wbench$MASE_ratio3<- phase2_exp_wbench$'Benchmark 3'/phase2_exp_wbench$MASE1_w2

#here we use logs, because skewness suggests that sqrt is not enough and logs do a good job across all three markers

model.phase2.hist.mean.ratio<-  lmer(log(MASE_ratio1)~domain+(1|team_name), data=phase2_exp_wbench)
Anova(model.phase2.hist.mean.ratio, test.statistic = "F")
#here are the results of the historical mean tests for Tournament 2
phase2.hist.mean.ratio.means<-as.data.frame(emmeans(model.phase2.hist.mean.ratio,~domain, type="response"))
phase2.hist.mean.ratio.means$Estimate<-"Historical Mean"
phase2.hist.mean.ratio.means$Date<-"Tournament 2 (Nov 2020)"
phase2.hist.mean.ratio.means

plot.t2.hist.mean<-plot(emmeans(model.phase2.hist.mean.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(name="")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="Tournament 2 (Nov 2020)")+theme(axis.text.y=element_blank())

model.phase2.randwalk.ratio<-  lmer(log(MASE_ratio2)~domain+(1|team_name), data=phase2_exp_wbench)

Anova(model.phase2.randwalk.ratio, test.statistic = "F")
phase2.randwalk.ratio.means<-as.data.frame(emmeans(model.phase2.randwalk.ratio,~domain, type="response"))
phase2.randwalk.ratio.means$Estimate<-"Random Walk"
phase2.randwalk.ratio.means$Date<-"Tournament 2 (Nov 2020)"
phase2.randwalk.ratio.means

#here are the results of the random walk tests for Tournament 2

plot.t2.randwalk<-plot(emmeans(model.phase2.randwalk.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(name="")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="")+theme(axis.text.y=element_blank())

model.phase2.linreg.ratio<-  lmer(log(MASE_ratio3)~domain+(1|team_name), data=phase2_exp_wbench)
Anova(model.phase2.linreg.ratio, test.statistic = "F")
phase2.linreg.ratio.means<-as.data.frame(emmeans(model.phase2.linreg.ratio,~domain, type="response"))
phase2.linreg.ratio.means$Estimate<-"Linear Regression"
phase2.linreg.ratio.means$Date<-"Tournament 2 (Nov 2020)"
phase2.linreg.ratio.means

#here are the results of the linear regression tests for Tournament 2


plot.t2.linreg<-plot(emmeans(model.phase2.linreg.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(name="")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="")+theme(axis.text.y=element_blank())

##examine if better than all 3 benchmarks

phase2_exp_wbench$phase2_exp_wbench_topBenchmark <- pmin(phase2_exp_wbench$'Benchmark 1',phase2_exp_wbench$'Benchmark 2',phase2_exp_wbench$'Benchmark 3')
phase2_exp_wbench$MASE_ratio2_topBenchmark<- phase2_exp_wbench$phase2_exp_wbench_topBenchmark/phase2_exp_wbench$MASE1_w2

#skewness test suggests that sqrt is the most reasonable transformation  hence we will use it.
model.phase2.topBenchmark.ratio<-  lmer(sqrt(MASE_ratio2_topBenchmark)~domain+(1|team_name), data=phase2_exp_wbench)
Anova(model.phase2.topBenchmark.ratio, test.statistic = "F")
emmeans(model.phase2.topBenchmark.ratio,~domain, type="response")


plot.t2.topBenchmark<-plot(emmeans(model.phase2.topBenchmark.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(labels=labeling, name="Top Naive Benchmark")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="Tournament 2 (Nov 2020)")


#combine all graphs
figs2<-ggarrange(plot.t1.hist.mean,plot.t2.hist.mean,
                     plot.t1.randwalk,plot.t2.randwalk, 
                     plot.t1.linreg, plot.t2.linreg,  ncol=2, nrow=3,widths=c(1.3,1))

figs2

####REDO figure in ggplot, sorting the facets differently
# Combine the datasets of means
mean.scores.w.benchmarks <- rbind(phase1.hist.mean.ratio.means,phase1.randwalk.ratio.means,phase1.linreg.ratio.means,
                     phase2.hist.mean.ratio.means,phase2.randwalk.ratio.means,phase2.linreg.ratio.means) 

#### Figure 2 in the manuscript
mean.scores.w.benchmarks %>% ggplot(aes(x = response, y = domain, colour = Estimate, fill=Estimate))+
 geom_pointrange(aes(xmin=lower.CL, xmax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_vline(xintercept =1, linetype='dashed', color='red', 14)+facet_grid(~Date, scales = "free_y")+
theme(legend.position="bottom") +scale_color_aaas()+scale_fill_aaas()+  scale_y_discrete(labels=labeling, name="")+
  labs(colour = "",fill="", y="",x="NaÃ¯ve Benchmark / Scientific Forecast Error Ratio (M +/- 95%CI)") 
```

## compare scores from tournament 1 and tournament 2 
## test complexity associations

```{r}
#count how many domains per person
phase1_exp<-phase1_exp %>%group_by(team_name) %>%  mutate(n_domains = n())
phase1_exp$Domain_Publications<-ifelse(phase1_exp$pub==1,1,ifelse(phase1_exp$pub==2,0,NA))


#count how many domains per person
dat_phase2<-dat_phase2 %>%group_by(team_name) %>%  mutate(n_domains = n())
dat_phase2$Domain_Publications<-ifelse(dat_phase2$pub==1,1,ifelse(dat_phase2$pub==2,0,NA))

#####################
#create subsets for tournament 1 and for tournament 2 to use in analyses here and later for covariate analyses below
#####################
subset1<- phase1_exp %>% ungroup() %>% dplyr::select(MASE1_w1,domain,Method.code,ResponseId,team_name,covidcondyn,CounterFactual_Presence_Final,Method.complex,parameters_coded,n_domains,multi_dis.factor,team_discipline.coded,non_US,team_size.coded,team_gender,team_education,confidence,subexpert,Domain_Publications,previous_tournament.coded,TournamentStart) %>% mutate(inaccuracy = MASE1_w1,phase = "first")

subset2<- dat_phase2 %>% ungroup() %>% dplyr::select(MASE1_w2,domain,Method.code,ResponseId,team_name,covidcondyn,CounterFactual_Presence_Final,Method.complex,parameters_coded,n_domains,multi_dis.factor,team_discipline.coded,non_US,team_size.coded,team_gender,team_education,confidence,subexpert,Domain_Publications,previous_tournament.coded,TournamentStart) %>% mutate(inaccuracy = MASE1_w2,phase = "second")

##compare effects by domain for each tournament
##REPORTED IN MAIN TEXT####
subset1.model<-  lmer(log(inaccuracy)~domain+(1|team_name), data=subset1)
car::Anova(subset1.model,type="III", test.statistic="F") #sig effect
emmeans(subset1.model,~domain, type="response")
partR2(subset1.model)
#rsq = 0.4498

subset2.model<-  lmer(log(inaccuracy)~domain+(1|team_name), data=subset2)
car::Anova(subset2.model,type="III", test.statistic="F") #sig effect
emmeans(subset2.model,~domain, type="response")
partR2(subset2.model)
#rsq = 0.2909

###########################

##############################
#combine tournament 1 and tournament 2 subsets for later analyses with covariates
#BEGINNING
##############################
both.sets<-bind_rows(subset1,subset2)

both.sets$covidconditional<-ifelse(both.sets$covidcondyn==1,1,0)
both.sets$covidconditional[is.na(both.sets$covidconditional)]<-0
both.sets$Method.complex[is.na(both.sets$Method.complex)]<-1 #simple when no extra info is provided, because the rest {number of parameters et.) suggests no extra factors considered}
both.sets$multi_dis.factor[is.na(both.sets$multi_dis.factor)]<-"Single domain expertise" #(setting is NA to non multidisciplinary)
both.sets$team_discipline.coded[is.na(both.sets$team_discipline.coded)]<-5 #(setting is NA to other)

both.sets$team_discipline.datasci<-ifelse(both.sets$team_discipline.coded==3,1,0)
both.sets$team_discipline.SBsci<-ifelse(both.sets$team_discipline.coded==1,1,ifelse(both.sets$team_discipline.coded==2,1,0))

#add complexity

both.sets<-both.sets %>% left_join(complexity)
both.sets$sd_hist<-ifelse(both.sets$phase=="first",both.sets$sd_hist_w1,both.sets$sd_hist_w2)
both.sets$mad_hist<-ifelse(both.sets$phase=="first",both.sets$mad_hist_w1,both.sets$mad_hist_w2)
both.sets$perp_entropy_hist<-ifelse(both.sets$phase=="first",both.sets$perp_entropy_hist_w1,both.sets$perp_entropy_hist_w2)

both.sets$sd<-ifelse(both.sets$phase=="first",both.sets$sd_w1,both.sets$sd_w2)
both.sets$mad<-ifelse(both.sets$phase=="first",both.sets$mad_w1,both.sets$mad_w2)
both.sets$perp_entropy<-ifelse(both.sets$phase=="first",both.sets$perp_entropy_w1,both.sets$perp_entropy_w2)

#add domain differences in complexity between waves (just supplementary interests)
both.sets$sd_hist_diff<-both.sets$sd_hist_w2-both.sets$sd_hist_w1
both.sets$mad_hist_diff<-both.sets$mad_hist_w2-both.sets$mad_hist_w1
both.sets$perp_entropy_hist_diff<-both.sets$perp_entropy_hist_w2-both.sets$perp_entropy_hist_w1

both.sets$sd_diff<-both.sets$sd_w2-both.sets$sd_w1
both.sets$mad_diff<-both.sets$mad_w2-both.sets$mad_w1
both.sets$perp_entropy_diff<-both.sets$perp_entropy_w2-both.sets$perp_entropy_w1
##############################
#combine tournament 1 and tournament 2 subsets for later analyses with covariates
#END
##############################

#############################
#analyze comparison of tournament 1 to tournament 2, REPORTED IN THE MAIN TEXT
#BEGINNING
#############################

both.sets.model<-  lmer(log(inaccuracy)~phase+(1|team_name), data=both.sets)
car::Anova(both.sets.model,type="III", test.statistic="F") #sig effect
emmeans(both.sets.model,~phase, type="response")
partR2(both.sets.model)
#effect size part rsq  0.0628

#####################
#supplementary analyses - comparison of tournament 1 versus tournament 2 by domain
#####################
both.sets.model.by.domain<-  lmer(log(inaccuracy)~phase*domain+(1|team_name), data=both.sets)
car::Anova(both.sets.model.by.domain,type="III", test.statistic="F") #sig effect
emmeans(both.sets.model.by.domain,~phase|domain, type="response")
emmeans(both.sets.model.by.domain,pairwise~phase|domain, type="response")
t.comparison.effects<-as.data.frame(emmeans(both.sets.model.by.domain,pairwise~phase|domain, type="response")$emmeans)
t.comparison<-as.data.frame(emmeans(both.sets.model.by.domain,pairwise~phase|domain, type="response")$contrasts)
#####################

#####################
#analyses of tournament 1 versus tournament 2 with covariates
#####################
both.sets.model.cov<-  lmer(log(inaccuracy)~phase+domain+
                              n_domains+team_discipline.datasci+team_discipline.SBsci+multi_dis.factor+team_size.coded+team_gender+team_education+Domain_Publications+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(both.sets.model.cov,type="III", test.statistic="F") #sig effect
emmeans(both.sets.model.cov,~phase, type="response")
partR2(both.sets.model.cov, partvars = 
         c("phase","domain"))
#ergo, part rsq for phase itself remains 0.0617

```


#graph change in ranking
```{r}
##MAIN TEXT FIGURE SHOWING RANKING AND DOMAIN'S MASE, AS WELL AS SHOWING WHICH DIFFERENCES ARE SIG

#get ranking of scores among academics in May and November
median.MASE.t1$phase<-"first"
median.MASE.t2$phase<-"second"
median.MASE.t1$Wave<-"First Tournament\nMay 2020"
median.MASE.t2$Wave<-"Second Tournament\nNov 2020"
median.ranks<-bind_rows(median.MASE.t1,median.MASE.t2)
median.ranks$Domain[median.ranks$domain=="eafric"]<-"Exp. Afr.-Am. Bias"
median.ranks$Domain[median.ranks$domain=="easian"]<-"Exp. Asian-Am. Bias"
median.ranks$Domain[median.ranks$domain=="egend"]<-"Exp. Gender Bias"
median.ranks$Domain[median.ranks$domain=="iafric"]<-"Imp. Afr.-Am. Bias"
median.ranks$Domain[median.ranks$domain=="iasian"]<-"Imp. Asian-Am. Bias"
median.ranks$Domain[median.ranks$domain=="ideoldem"]<-"Democrat. Support"
median.ranks$Domain[median.ranks$domain=="ideolrep"]<-"Republic. Support"
median.ranks$Domain[median.ranks$domain=="igend"]<-"Imp. Gender Bias"
median.ranks$Domain[median.ranks$domain=="lifesat"]<-"Life Satisfaction"
median.ranks$Domain[median.ranks$domain=="polar"]<-"Polarization"
median.ranks$Domain[median.ranks$domain=="negaffect"]<-"Negative Affect"
median.ranks$Domain[median.ranks$domain=="posaffect"]<-"Positive Affect"

median.ranks<-median.ranks %>% left_join(t.comparison.effects) %>% left_join(t.comparison %>% dplyr::select(domain,ratio,t.ratio,p.value)) #add the sig testing from the tournament comparisons, incl ratio size and p-values

#NOTE: here we have median ranks per domain, but also the estimates scores from multi-level models accounting for multiple predictions by different scientist groups. Due to this dependence in the data, we use the latter estimates.


### Figure 3 in the manuscript
median.ranks$sig<-ifelse(median.ranks$p.value<.05,"eff","noeff")
median.ranks$MASE<-round(median.ranks$response,2) #two decimals
median.ranks$ranksize<-round(median.ranks$ratio,2) #two decimals
newggslopegraph(dataframe = median.ranks,
                Times = Wave,
                Measurement = MASE,
                Grouping = Domain,LineThickness = 2,
                WiderLabels=T,
                Title = "Which domains are harder to predict?",TitleJustify = "center",
                SubTitle = NULL,
                Caption = "Ranking based on MASE scores per domain",
                ThemeChoice="ipsum")+scale_color_d3(palette = "category20")+geom_line(aes(linetype=sig, color="black",alpha=1))
```

# ranking of domain by forecasting error and correlation to historical variability in trends

```{r ranking in phase 1 and complexity}
##test reported in MAIN TEXT showing that domain differences in forecasting accuracy corresponded to differences in the complexity of historical data

#covert to wide
median.ranks.wide<-median.ranks %>% dplyr::select(domain:phase,MASE) %>% 
  pivot_wider(names_from="phase",values_from=c("MASE","MASE_med"))

rank_complex_wide<-median.ranks.wide%>%left_join(complexity)

#For complexity,we used SD and MAD. Additionally, I consider permutation_entropy, which is Ra substitution the Shannon entropy with a monoparametric entropy. 

rank_complex_w1<-median.MASE.t1%>%left_join(complexity)
rank_complex_w2<-median.MASE.t2%>%left_join(complexity)

rank_complex<-median.ranks%>%left_join(complexity)

#the scores below include both complexity markers (SD, MAD, and supplementary entropy) for historical data, as well as the data across the 12 months of the tournament. For the main text analyses we focus on historical complexity
correlation::correlation(rank_complex%>%filter(phase=="first") %>% 
 dplyr::select(MASE_med,sd_hist_w1,mad_hist_w1,perp_entropy_hist_w1,sd_w1,mad_w1,perp_entropy_w1), p_adjust="none", ranktransform=T) 

correlation::correlation(rank_complex%>%filter(phase=="second") %>% 
 dplyr::select(MASE_med,sd_hist_w2,mad_hist_w2,perp_entropy_hist_w2,sd_w2,mad_w2,perp_entropy_w2), p_adjust="none", ranktransform=T) 

#add difference scores in complexity to the dataset
#these are additional analyses to check the differences in complexity and differences in accuracy between tournaments

rank_complex$sd_hist_diff<-rank_complex$sd_hist_w2-rank_complex$sd_hist_w1
rank_complex$mad_hist_diff<-rank_complex$mad_hist_w2-rank_complex$mad_hist_w1
rank_complex$perp_entropy_hist_diff<-rank_complex$perp_entropy_hist_w2-rank_complex$perp_entropy_hist_w1

rank_complex$sd_diff<-rank_complex$sd_w2-rank_complex$sd_w1
rank_complex$mad_diff<-rank_complex$mad_w2-rank_complex$mad_w1
rank_complex$perp_entropy_diff<-rank_complex$perp_entropy_w2-rank_complex$perp_entropy_w1

rank_complex$wave<-ifelse(rank_complex$phase=="first",0,1)

domain.SD.change<-  lmer(MASE~wave*sd_hist_diff+(1|Domain), data=rank_complex)
car::Anova(domain.SD.change,type="III", test.statistic="F") #sig effect
emtrends(domain.SD.change,specs=~wave,var="sd_hist_diff") #
interactions::sim_slopes(domain.SD.change,pred="wave",modx="sd_hist_diff", digits=4)
#this interaction shows that when change in SD is high (domains become more variable at t2 compared to t1), there is no difference in inaccuracy
 
domain.MAD.change<-  lmer(MASE~wave*mad_hist_diff+(1|Domain), data=rank_complex)
car::Anova(domain.MAD.change,type="III", test.statistic="F") #sig effect
emtrends(domain.MAD.change,specs=~wave,var="mad_hist_diff") #
interactions::sim_slopes(domain.MAD.change,pred="wave",modx="mad_hist_diff", digits=4)
#this interaction shows that when change in MAD is high (domains become more variable at t2 compared to t1), there is no difference in inaccuracy between t1 and t2

##examine difference scores via corr

rank_complex_wide$sd_hist_diff<-rank_complex_wide$sd_hist_w2-rank_complex_wide$sd_hist_w1
rank_complex_wide$mad_hist_diff<-rank_complex_wide$mad_hist_w2-rank_complex_wide$mad_hist_w1
rank_complex_wide$perp_entropy_hist_diff<-rank_complex_wide$perp_entropy_hist_w2-rank_complex_wide$perp_entropy_hist_w1

rank_complex_wide$sd_diff<-rank_complex_wide$sd_w2-rank_complex_wide$sd_w1
rank_complex_wide$mad_diff<-rank_complex_wide$mad_w2-rank_complex_wide$mad_w1
rank_complex_wide$perp_entropy_diff<-rank_complex_wide$perp_entropy_w2-rank_complex_wide$perp_entropy_w1

rank_complex_wide$MASE_diff<-rank_complex_wide$MASE_second-rank_complex_wide$MASE_first
rank_complex_wide$MASE_med_diff<-rank_complex_wide$MASE_med_second-rank_complex_wide$MASE_med_first


correlation::correlation(rank_complex_wide%>%
 dplyr::select(MASE_diff,MASE_med_diff,sd_hist_diff,mad_hist_diff,perp_entropy_hist_diff,sd_diff,mad_diff,perp_entropy_diff), p_adjust="none", ranktransform=T) 
#added info about change in variability correlating to changes in accuracy.

```


## compare scores from tournament 1 - first six months vs. last six months
```{r}
#turn to long format (firstmonths and lastmonths MASE data)
data.t1.t2.exp.long<- phase1_exp %>% ungroup() %>%pivot_longer(cols=c("MASE1_w1_firstmonths","MASE1_w1_lastmonths"), names_to="Time",values_to="MASE") 
model.t1.t2<-lmer(log(MASE)~Time*domain+(1|team_name), data=data.t1.t2.exp.long)

summ(model.t1.t2)     
car::Anova(model.t1.t2,type="III",test.statistic="F") #sig interaction!
emmeans(model.t1.t2,~Time, , type="response")
partR2(model.t1.t2, partvars = 
         c("Time","domain"))

#NEXT ANALYSES: compare last months from the T1 to T2
##turn to long format (lastmonths T1 an t2 MASE data)
subset1.lastsix<- phase1_exp %>% ungroup() %>% dplyr::select(MASE1_w1_lastmonths,domain,Method.code,ResponseId,team_name,covidcondyn,CounterFactual_Presence_Final,Method.complex,parameters_coded,n_domains,multi_dis.factor,team_discipline.coded,team_size.coded,team_gender,team_education,confidence,subexpert,Domain_Publications,previous_tournament.coded, TournamentStart) %>% mutate(inaccuracy = MASE1_w1_lastmonths,phase = "first")

#combine
both.sets.lastsix<-bind_rows(subset1.lastsix,subset2)
#test
model.t1.t2.lastsix<-lmer(log(inaccuracy)~phase+domain+(1|team_name), data=subset(both.sets.lastsix,TournamentStart=="May"))
  
car::Anova(model.t1.t2.lastsix,type="III",test.statistic="F") 
emmeans(model.t1.t2.lastsix,specs = trt.vs.ctrl ~phase,  type="response",adjust = "fdr") 
partR2(model.t1.t2.lastsix, partvars = 
         c("phase","domain"))
```

# Consistency in forecasting
```{r inaccuracy on odd and even month - stability of inaccuracy}
#to assess and protect against the possibility that forecasting models are accurate by chance (in the same way that some investing strategies can âget luckyâ in a particular time point without actually being better than other strategies), we used subsets of the data (odd and event months) to determine whether model accuracy in one subset of predictions (ranking of model performance across odd months) correlates with model accuracy in the other subset (ranking of model performance across even months). 

dat_long_phase1_exp<-(subset(dat_long_phase1, isExpert.factor == 'Academic'))

dat_long_phase1_exp_wide_by_month<-dat_long_phase1_exp %>%dplyr::select(domain,team_name,Month,value.dif) %>%  pivot_wider(names_from=c(Month),values_from=c(value.dif))

dat_long_phase1_exp_wide_by_month$odd_month_inaccuracy=rowMeans(dat_long_phase1_exp_wide_by_month[c("1","3","5","7","9","11")],na.rm=T)

dat_long_phase1_exp_wide_by_month$even_month_inaccuracy=rowMeans(dat_long_phase1_exp_wide_by_month[c("2","4","6","8","10","12")],na.rm=T)

#correlations by domain

dat_long_phase1_exp_wide_by_month %>%dplyr::select(domain,odd_month_inaccuracy,even_month_inaccuracy) %>% 
    group_by(domain) %>%
    correlation::correlation(ranktransform =T)    

#multilevel
dat_long_phase1_exp_wide_by_month %>%dplyr::select(domain,odd_month_inaccuracy,even_month_inaccuracy) %>% 
    correlation::correlation(multilevel=T,ranktransform=T)  


###PHASE 2

dat_long$Month7<-dat_long$Month-7
dat_long_phase2<-dat_long %>%filter(!(phase == 1 & revised == 1)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" & Month %in% c(7,8,9,10,11,12))



dat_long_phase2_exp_wide_by_month<-dat_long_phase2 %>%dplyr::select(domain,team_name,Month7,value.dif) %>%  pivot_wider(names_from=c(Month7),values_from=c(value.dif))

dat_long_phase2_exp_wide_by_month$even_month_inaccuracy=rowMeans(dat_long_phase2_exp_wide_by_month[c("1","3","5")],na.rm=T) # Evie: I think these are even months since previously the month was subtracted by 7

dat_long_phase2_exp_wide_by_month$odd_month_inaccuracy=rowMeans(dat_long_phase2_exp_wide_by_month[c("2","4","0")],na.rm=T) # Evie: I think these are odd months since previously the month was subtracted by 7

#correlations by domain

dat_long_phase2_exp_wide_by_month %>%dplyr::select(domain,odd_month_inaccuracy,even_month_inaccuracy) %>% 
    group_by(domain) %>%
    correlation::correlation(ranktransform =T)    

#multilevel
dat_long_phase2_exp_wide_by_month %>%dplyr::select(domain,odd_month_inaccuracy,even_month_inaccuracy) %>% 
    correlation::correlation(multilevel=T,ranktransform=T) 


```

## visualize by method (phase 1 and 2)

```{r}
#analyses of phase 1  - MASE overall
#For models evaluating overall accuracy of the forecasted model, we will use forecasting type (purely theoretical, purely data-driven and hybrid models), forecasting domain as predictors, with MASE scores nested within teams. 

#first, data the proper subset for Tournament 2
dat_phase2<-academic_only %>%filter(!(phase == 1 & revised == 1)) #just academics


######################################
#what is the percentage using different method?

#Tournament 1: 
prop.table(table(phase1_exp$Method.code))
#Tournament 2: 
prop.table(table(dat_phase2$Method.code))
table(phase2$Method.code)

#SUPPLEMENTARY FIGURE showing differences in percentages of each category by domain
######################################
perc.by.domain.phase1<-phase1_exp %>%
  group_by(domain,Method.code) %>%
  summarise(n = n()) %>%
  mutate(perc = round(n / sum(n)*100),2) %>% 
  ggplot(aes(x = "", y = perc, fill = Method.code)) +
  geom_col(color = "black") +
  geom_label(aes(label = perc),
             color = "white",
             position = position_stack(vjust = 0.5),
             show.legend = FALSE) +scale_fill_jama()+labs(fill="")+
  coord_polar(theta = "y")+theme_void()+facet_wrap(~domain, nrow = 4, labeller=labeller(domain=labels))+theme(legend.position="bottom")

perc.by.domain.phase1

#Tournament 2

perc.by.domain.phase2<-dat_phase2 %>%
  group_by(domain,Method.code) %>%
  summarise(n = n()) %>%
  mutate(perc = round(n / sum(n)*100),2) %>% 
  ggplot(aes(x = "", y = perc, fill = Method.code)) +
  geom_col(color = "black") +
  geom_label(aes(label = perc),
             color = "white",
             position = position_stack(vjust = 0.5),
             show.legend = FALSE) +scale_fill_jama()+labs(fill="")+
  coord_polar(theta = "y")+theme_void()+facet_wrap(~domain, nrow = 4, labeller=labeller(domain=labels))+theme(legend.position="bottom")

#combine plots
#plot percentages of different forecasting method choices by domain for tournament 1 and tournament 2 (i.e., put them together)
### Fig. S3 in the manuscript

cowplot::plot_grid(perc.by.domain.phase1,perc.by.domain.phase2,labels=c("1st Tournament","2nd Tournament"), label_size = 10,
  align = "v")
####################################

####################################
#examine analyses of forecasting method choice on accuracy
####################################
#recorder levels of the domains (to use later)
dat_long$domain <- factor(dat_long$domain,      # Reordering group factor levels
                         levels = c("egend","easian","eafric",
                                    "igend","iasian","iafric",
                                    "posaffect","negaffect","lifesat",
                                    "polar","ideoldem","ideolrep"))
#Tournament 1: run models
model.phase1.across.domains<-  lmer(log(MASE1_w1)~Method.code+domain+(1|ResponseId), data=subset(phase1,isExpert.factor=="Academic"))
car::Anova(model.phase1.across.domains,type="III", test.statistic="F") 
partR2(model.phase1.across.domains, partvars = 
         c("Method.code","domain"))
data.phase1.MASE.total<-as.data.frame(emmeans(model.phase1.across.domains,pairwise ~Method.code, type = "response", adjust = "none")$emmeans)

#next run Tournament 2 models
data.phase2.model.across.domains<-  lmer(log(MASE1_w2)~Method.code * domain+(1|ResponseId), data=dat_phase2)
car::Anova(data.phase2.model.across.domains,type="III", test.statistic="F") #sig interaction!
data.phase2.MASE.total<-as.data.frame(emmeans(data.phase2.model.across.domains,pairwise ~Method.code, type = "response", adjust = "none")$emmeans)
partR2(data.phase2.model.across.domains, partvars = 
         c("Method.code","domain"))

## we test if forecasts that considered historical data as part of the forecast modelling were more accurate than models that did not - MAIN TEXT
#i.e., EXAMINE ONLY ACADEMICS, USING CONTRAST OF THEORY vs. DATA.HYBRID
### Tournament 1
phase1_exp$method.contrast<-ifelse(phase1_exp$Method.code=='Intuition/Theory',0,1)
model.phase1.contrast<-  lmer(log(MASE1_w1)~method.contrast+domain+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.contrast,type="III", test.statistic="F") #sig domain effect,  and sig interaction
summ(model.phase1.contrast, digits=4) #get effect size for the overall model
partR2(model.phase1.contrast, partvars = 
         c("method.contrast","domain"))

### Tournament 2
dat_phase2$method.contrast<-ifelse(dat_phase2$Method.code=='Intuition/Theory',0,1)
model.phase2.contrast<-  lmer(log(MASE1_w2)~method.contrast+domain+(1|ResponseId), data=dat_phase2)
car::Anova(model.phase2.contrast,type="III", test.statistic="F") #sig domain effect,  and sig interaction
summ(model.phase2.contrast, digits=4) #get effect size for the overall model
partR2(model.phase2.contrast, partvars = 
         c("method.contrast","domain"))

## Test if model comparison effects were qualified by significant model type X domain interaction
### Tournament 1
model.phase1.contrast.by.domain<-  lmer(log(MASE1_w1)~method.contrast*domain+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.contrast.by.domain, type=3, test.statistic="F")
data.phase1.MASE.method<-as.data.frame(emmeans(model.phase1.contrast.by.domain,pairwise ~method.contrast|domain, type="response")$contrasts)

#get FDR correction across all pairwise tests
data.phase1.MASE.method$Hochberg <-p.adjust(data.phase1.MASE.method$p.value,
               method = "hochberg")
data.phase1.MASE.method

partR2(model.phase1.contrast.by.domain, partvars = 
         c("method.contrast","domain:method.contrast","domain"))

write.csv(data.phase1.MASE.method,"contrast1.csv") #for the table in supplement

### Tournament 2
model.phase2.contrast.by.domain<-  lmer(log(MASE1_w2)~method.contrast*domain+(1|ResponseId), data=dat_phase2)
car::Anova(model.phase2.contrast.by.domain, type=3, test.statistic="F")

data.phase2.MASE.method<-as.data.frame(emmeans(model.phase2.contrast.by.domain,pairwise ~method.contrast|domain, type="response")$contrasts)

#get FDR correction across all pairwise tests
data.phase2.MASE.method$Hochberg <-p.adjust(data.phase2.MASE.method$p.value,
               method = "hochberg")
data.phase2.MASE.method

partR2(model.phase2.contrast.by.domain, partvars = 
         c("method.contrast","domain:method.contrast","domain"))
write.csv(data.phase2.MASE.method,"contrast2.csv") #for table in the supplement

## supplementary model with all three forecasting methods*domain interaction is below. We use it to get estimates for modelling by domain by method
### Tournament 1
model.phase1<-  lmer(log(MASE1_w1)~domain*Method.code+(1|ResponseId), data=subset(phase1,isExpert.factor=="Academic"))
car::Anova(model.phase1,type="III") #sig interaction!
summ(model.phase1, digits=4)
emmeans(model.phase1,trt.vs.ctrl ~Method.code|domain, adjust = "none") #lay vs. rest
emmeans(model.phase1,pairwise ~Method.code|domain, adjust = "none")
partR2(model.phase1, partvars = 
         c("domain","domain:Method.code","domain"))
data.phase1.MASE<-as.data.frame(emmeans(model.phase1,pairwise ~Method.code|domain, type = "response", adjust = "none")$emmeans) 

### Tournament 2
data.phase2.model<-  lmer(log(MASE1_w2)~domain*Method.code+(1|ResponseId), data=dat_phase2)
car::Anova(data.phase2.model,type="III") #sig interaction!
partR2(data.phase2.model, partvars = 
         c("domain","domain:Method.code","domain"))
data.phase2.MASE<-as.data.frame(emmeans(data.phase2.model, pairwise~Method.code|domain, adjust = "none", type = "response")$emmeans) #backtransformed to the original scale

## Supplementary analyses to examine if data-free forecasts of social scientists were not better than lay estimates, in Tournament 1
###EXAMINE ONLY ACADEMICS, USING CONTRAST OF THEORY vs. DATA.HYBRID
phase1$method.contrast.layppl[phase1$Method.code=='Intuition/Theory']<-"Sci data-free"
phase1$method.contrast.layppl[phase1$Method.code=='Lay People']<-"lay people"
phase1$method.contrast.layppl[phase1$Method.code=='Data-Driven']<-"Sci data-incl."
phase1$method.contrast.layppl[phase1$Method.code=='Hybrid']<-"Sci data-incl."

phase1$MASE1_w1_log<-log(phase1$MASE1_w1) #this this to get emmeans-based effect size Cohen's d for pairwise comparisons

model.phase1.contrast.lay<-  lmer(MASE1_w1_log~method.contrast.layppl+(1|ResponseId), data=phase1)
car::Anova(model.phase1.contrast.lay,type="III") #sig domain effect,  and sig interaction
emmeans(model.phase1.contrast.lay,specs = trt.vs.ctrl ~method.contrast.layppl, adjust = "fdr",type="response" ) 
#significant difference between academics who used data and lay people, but not between academics who did not use data and lay people
( EMM = emmeans(model.phase1.contrast.lay, "method.contrast.layppl") )
pairs(EMM)
eff_size(EMM,sigma = sigma(model.phase1.contrast.lay), edf =df.residual(model.phase1.contrast.lay) ) #using the smallest DF among the three

############SOME EXTRA FIGURES: NOT USED IN THE MANUSCRIPT OR SUPPLEMENT##################
#########BEGINNING###################
###Tournament 1
#arrange in descending order based on MASE w2 of academics
data.phase1.MASE$domain<-factor(data.phase1.MASE$domain,levels=c("ideolrep","negaffect","ideoldem","polar","iafric","lifesat","eafric","easian","egend","iasian","igend","posaffect"))

data.phase1.MASE %>% 
 ggplot(aes(x = domain, y = response, colour = Method.code, fill=Method.code))+
 geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+coord_flip()+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labeling, name="")+
  labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)") 

data.phase1.MASE.total %>% 
 ggplot(aes(x = Method.code, y = response, colour = Method.code, fill=Method.code))+
 geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+coord_flip()+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labeling, name="")+
  labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)") 
### Tournament 2
data.phase2.MASE %>% 
 ggplot(aes(x = domain, y = response, colour = Method.code, fill=Method.code))+
 geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+coord_flip()+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labeling, name="")+
  labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)") 

data.phase2.MASE.total %>% 
 ggplot(aes(x = Method.code, y = response, colour = Method.code, fill=Method.code))+
 geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+coord_flip()+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labeling, name="")+
  labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)") 
############SOME EXTRA FIGURES: NOT USED IN THE MANUSCRIPT OR SUPPLEMENT##################
#########END###################

###CREATE FIGURE FOR THE MAIN TEXT
data.phase1.MASE.total$Wave<-"First Tournament (May 2020)"
data.phase2.MASE.total$Wave<-"Second Tournament (Nov 2020)"
#combine
means.compare.by.method<-bind_rows(data.phase1.MASE.total,data.phase2.MASE.total)
means.compare.by.method$Method<-means.compare.by.method$Method.code #create a copy to port values to
means.compare.by.method$Method<-c('Data-Driven\n51%','Hybrid\n7%','Intuition/\nTheory\n42%','Data-Driven\n53%','Hybrid\n8%','Intuition/\nTheory\n39%')
#arrange in descending order based on MASE w2 of academics
means.compare.by.method$Wave<-factor(means.compare.by.method$Wave,levels=c("First Tournament (May 2020)","Second Tournament (Nov 2020)"))


#plot figure
### Figure 4 in the manuscript

means.compare.by.method %>%  
 ggplot(aes(x = Method, y = response, color = Method.code))+
 geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14)+geom_hline(yintercept =1, linetype='dotted', color='black',14)+
  geom_hline(yintercept =1.7665, linetype='dashed', color='blue',16)+theme(legend.position="none")+scale_color_futurama()+  labs(y="MASE (M +/- 95%CI)",x="",shape="",color="")+ facet_wrap(~ Wave, scales = "free_x")

  
```


## examine effects of covariates across both tournaments

```{r}
#examine effects of covariates

both.sets$inaccuracy_log<-log(both.sets$inaccuracy)

both.sets$Multidisciplinary<-ifelse(both.sets$multi_dis.factor=="Single domain expertise",0,1)
both.sets$covidconditional[is.na(both.sets$covidconditional)]<-0
both.sets$Method.complex[is.na(both.sets$Method.complex)]<-1 #simple when no extra info is provided, because the rest {number of parameters et.) suggests no extra factors considered}
both.sets$multi_dis.factor[is.na(both.sets$multi_dis.factor)]<-"Single domain expertise" #(setting is NA to non multidisciplinary)

###analyses with domain

model.bothTournaments.COVs<-lmer(inaccuracy_log~domain+parameters_coded+Method.complex+covidconditional+CounterFactual_Presence_Final+n_domains+team_discipline.datasci+team_discipline.SBsci+Multidisciplinary+team_size.coded+team_education+confidence+subexpert+Domain_Publications+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(model.bothTournaments.COVs,type="III",test.statistic="F") 

summ(model.bothTournaments.COVs, conf.method="boot", digits=5, center=T)
#Rsq = 0.31437
model.bothTournaments.no.COVs<-lmer(inaccuracy_log~domain+(1|team_name), data=both.sets)
summ(model.bothTournaments.no.COVs, conf.method="boot", digits=3, center=T)


#xtra analysis with US residents on the team - not included to avoid overfitting.
model.bothTournaments.COVs.incl.US<-lmer(inaccuracy_log~domain+parameters_coded+Method.complex+covidconditional+CounterFactual_Presence_Final+n_domains+team_discipline.datasci+team_discipline.SBsci+Multidisciplinary+non_US+team_size.coded+team_education+confidence+subexpert+Domain_Publications+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(model.bothTournaments.COVs.incl.US,type="III",test.statistic="F") 
#no effect of US residency

#xtra analysis without objective expertise to examine partial Rsq 

model.bothTournaments.COVs.no.obj.expertise<-lmer(inaccuracy_log~domain+parameters_coded+Method.complex+covidconditional+CounterFactual_Presence_Final+n_domains+team_discipline.datasci+team_discipline.SBsci+Multidisciplinary+team_size.coded+team_education+confidence+subexpert+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(model.bothTournaments.COVs.no.obj.expertise,type="III",test.statistic="F") 

summ(model.bothTournaments.COVs.no.obj.expertise, conf.method="boot", digits=5, center=T)

#Rsq - 0.30449
anova(model.bothTournaments.COVs,model.bothTournaments.COVs.no.obj.expertise)

#flip accuracy and inaccuracy

both.sets$accuracy_log<-both.sets$inaccuracy_log*(-1)
model.bothTournaments.accuracy.COVs<-lmer(accuracy_log~domain+parameters_coded+Method.complex+covidconditional+CounterFactual_Presence_Final+n_domains+team_discipline.datasci+team_discipline.SBsci+Multidisciplinary+team_size.coded+team_education+confidence+subexpert+Domain_Publications+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(model.bothTournaments.accuracy.COVs,type="III",test.statistic="F") 

summ(model.bothTournaments.accuracy.COVs, conf.method="boot", digits=3, center=T)


plot.COV<-plot_summs(model.bothTournaments.accuracy.COVs, scale = TRUE, robust = "HC1",n.sd = 2, inner_ci_level = .9,
                     coefs = c("Statistical Model Complexity" = "Method.complex","N Model Parameters" = "parameters_coded", 
                     "Considered COVID-19" = "covidconditional",
                     "Considered Counterfactuals"="CounterFactual_Presence_Final",
                     "Number of Predicted Domains"="n_domains",
                     "Data Scientists on the Team"="team_discipline.datasci",
                     "Behav./Soc. Scientists on the Team"="team_discipline.SBsci",
                     "Multidisciplinary"="Multidisciplinary",
                     "Team Size"="team_size.coded",
                     "% without PhD on the Team"="team_education",
                     "Confidence in Forecast"="confidence",
                     "Confidence in Expertise"="subexpert",
                     "Team Members Topic Publications"="Domain_Publications",
                      "Prev. Exp. with\nForecasting Tournaments"="previous_tournament.coded"))
plot.COV$data<-plot.COV$data %>% arrange(estimate) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
  mutate(term=factor(term, levels=term))

### Figure 5 in the manuscript
  plot.COV+theme_pubclean()+labs(y="",x="Contribution to Accuracy",caption = "most negative <===========================================> most positive")
  
export_summs(model.bothTournaments.accuracy.COVs, scale = TRUE, robust = "HC1",n.sd = 2, to.file = "docx", file.name="indiv.differences.standartized.docx")
export_summs(model.bothTournaments.accuracy.COVs, scale = F, robust = "HC1", to.file = "docx", file.name="indiv.differences.unstandartized.docx")

partr2.COV<-partR2(model.bothTournaments.accuracy.COVs,  data=both.sets,
              R2_type = "marginal", nboot = 10, CI = 0.95)
summary(partr2.COV) #obtain effect sizes scores for unique predictors (incremental R2)

partR2(model.bothTournaments.accuracy.COVs, partvars = 
         c("Domain_Publications","previous_tournament.coded","Method.complex","Multidisciplinary"))
       
       
##supplementary  - examine COVID score inaccuracy as predictor - does inaccuracy in predictions depend on COVID-inaccuracy?
#IMPORTANT: only done in Phase 1
phase1_exp$inaccuracy_log<-log(phase1_exp$MASE1_w1)
model.t1.COVID<-lmer(inaccuracy_log~domain+log(MASE1_covid)+(1|team_name), data=phase1_exp)
car::Anova(model.t1.COVID,type="III",test.statistic="F") 
summ(model.t1.COVID, conf.method="boot", digits=3, center=T) #no significant effect of COVID prediction in accuracy on MASE in accuracy


#supplementary - test interaction between model complexity and phase

model.bothTournaments.accuracy.COVs.phase<-lmer(accuracy_log~phase+domain+phase*parameters_coded+phase*Method.complex+covidconditional+CounterFactual_Presence_Final+n_domains+team_discipline.datasci+team_discipline.SBsci+Multidisciplinary+team_size.coded+team_education+confidence+subexpert+Domain_Publications+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(model.bothTournaments.accuracy.COVs.phase,type="III",test.statistic="F") #no significant interaction between phase and complexity (to test the question of lower number of datapoint favors simpler forecasts)

#supplementary curiosity check - does % gender qualify confidence

model.confidence.gender<-lmer(confidence~phase+domain+team_gender+(1|team_name), data=both.sets)
car::Anova(model.confidence.gender,type="III",test.statistic="F") #no significant interaction between phase and complexity (to test the question of lower number of datapoint favors simpler forecasts)
summ(model.confidence.gender)

```

# Role of Updating - Phase 2

```{r}
#proportions of scientists who updated their forecasts
proportions(table(phase1_exp$revised,phase1_exp$domain),margin=2)

#set up the file for analyses
pd <- position_dodge(0.7) # move them .07 to the left and right
dat_phase2<-academic_only %>%filter(!(phase == 1 & revised == 1)) #just academics
dat_phase2$Method.code <- relevel(factor(dat_phase2$Method.code), "Intuition/Theory") #use lay people as a reference group
dat_phase2$method.contrast<-ifelse(dat_phase2$Method.code=='Intuition/Theory',0,1)
dat_phase2$compare_to_naive_rwf_MASE2.update<-ifelse(dat_phase2$compare_to_naive_rwf_MASE_w2!="Equal to Naive rwf",dat_phase2$compare_to_naive_rwf_MASE_w2,ifelse(dat_phase2$compare_to_naive_rwf_MASE_w2=="Equal to Naive rwf","Below Naive rwf",NA))
dat_phase2$compare_to_naive_linear_MASE2.update<-ifelse(dat_phase2$compare_to_naive_linear_MASE_w2!="Equal to Naive linear",dat_phase2$compare_to_naive_linear_MASE_w2,ifelse(dat_phase2$compare_to_naive_linear_MASE_w2=="Equal to Naive linear","Below Naive linear",NA))
dat_phase2$Group[dat_phase2$TournamentStart=="May"&dat_phase2$revised == 0]<-"Original May"
dat_phase2$Group[dat_phase2$TournamentStart=="November"&dat_phase2$revised == 0]<-"Original November"
dat_phase2$Group[dat_phase2$TournamentStart=="May"&dat_phase2$revised == 1]<-"Updated May"
dat_phase2$teamS<-as.factor(ifelse(dat_phase2$team_size.coded>=6,3,ifelse(dat_phase2$team_size.coded<6 & dat_phase2$team_size.coded>1,2,ifelse(dat_phase2$team_size.coded==1,1,NA))))
dat_phase2$is_multidisciplinary<-ifelse(dat_phase2$discipline=="Multi-disciplinary",1,0)
dat_phase2$objectivexpert<-ifelse(dat_phase2$pub==1,"Expert",ifelse(dat_phase2$pub==2,"Non Expert",NA))
dat_phase2$covidconditional<-ifelse(dat_phase2$covidcondyn==0,"No",ifelse(dat_phase2$covidcondyn==1,"Yes",NA))

#add historical variability data (as extra variable)
dat_phase2<-dat_phase2 %>% left_join(complexity)

#count how many domains per person
dat_phase2<-dat_phase2 %>%group_by(team_name) %>% 
 mutate(n_domains = n())

## EXAMINE EFFECTS OF new teams at phase 2 vs. OG teams who updated their forecasts: Just ACADEMICS
##revised  - Indicates whether or not the team has a matching submission in both phase 1 & 2 for the same domain

#MAIN TEXT ANALYSES####
model.phase2.update<-  lmer(log(MASE1_w2)~Group+(1|ResponseId), data=dat_phase2)
car::Anova(model.phase2.update,type="III") #sig difference between original May and original Nov, but not between updated May and original Nov
summ(model.phase2.update)
emmeans(model.phase2.update,pairwise ~Group, adjust = "none") #nonsig
#contrast difference of updating forecasts for explicit asian bias, life satisfaction, neg affect, polarization, pos affect
eff_size(emmeans(model.phase2.update,pairwise ~Group, adjust = "none"),sigma = sigma(model.phase2.update), edf =df.residual(model.phase2.update) ) 
#######################

###by type of justification (supplementary)
##first, create the variable
#Just new data as a reason for update
dat_phase2$Group.data[dat_phase2$Group!="Updated May"]<-dat_phase2$Group
dat_phase2$Group.data[dat_phase2$Group=="Updated May"&dat_phase2$justification_dataReceived==1]<-"Data"
dat_phase2$Group.data[dat_phase2$Group=="Updated May"&dat_phase2$justification_theoreticalInsight==1]<-"Theory"
dat_phase2$Group.data[dat_phase2$Group=="Updated May"&dat_phase2$justification_externalEvent==1]<-"Extra"

model.phase2.update.data<-  lmer(log(MASE1_w2)~Group.data+(1|ResponseId), data=dat_phase2)
car::Anova(model.phase2.update.data,type="III") # there is a significant difference in prediction error as a function of types of justifications.
emmeans(model.phase2.update.data,pairwise ~Group.data, adjust = "none") 

dat_phase2$MASE1_w2_log<-log(dat_phase2$MASE1_w2)
model.phase2.update.type<-  lmer(MASE1_w2_log~justification_dataReceived+justification_theoreticalInsight+justification_externalEvent+(1|team_name), data=dat_phase2)
car::Anova(model.phase2.update.type,type="III")  
summ(model.phase2.update.type,scale = F, robust = "HC1", digits=3, n.sd = 2)
#no sig differences
#export_summs(model.phase2.update.type, scale = T, robust = "HC1", n.sd = 2, to.file = "docx")
partR2(model.phase2.update.type, partvars = 
         c("justification_dataReceived","justification_theoreticalInsight","justification_externalEvent"))



```

# demographics Phase 1

```{r}

phase1.exp.dem<-phase1_exp %>% dplyr::select(team_name, team_size.coded, n_domains,team_Age,team_education,non_US,team_gender,revised) %>% dplyr::group_by(team_name) %>% 
summarize_all(~ mean(.x, na.rm = TRUE))

phase2.exp.dem<-dat_phase2 %>% dplyr::select(team_name, team_size.coded, n_domains,team_Age,team_education,non_US,team_gender,revised) %>% dplyr::group_by(team_name) %>% 
summarize_all(~ mean(.x, na.rm = TRUE))


#number of participants
sum(phase1.exp.dem$team_size.coded)
sum(phase2.exp.dem$team_size.coded)

#team size
psych::describe(phase1.exp.dem$team_size.coded)
psych::describe(phase2.exp.dem$team_size.coded)

#n forecasted domains
psych::describe(phase1.exp.dem$n_domains)
psych::describe(phase2.exp.dem$n_domains)

#% participating teams per domain (out of 86/120
phase1_exp %>% dplyr::select(team_name, domain) %>% dplyr::group_by(domain) %>% 
  summarize_all(~ n()) %>% mutate(perc = team_name/86*100)

dat_phase2 %>% dplyr::select(team_name, domain) %>% dplyr::group_by(domain) %>% 
  summarize_all(~ n()) %>% mutate(perc = team_name/120*100)

# % teams with expertise

phase1_exp %>% dplyr::select(team_name, domain,Domain_Publications) %>% dplyr::group_by(domain,Domain_Publications) %>% 
  summarize_all(~ n()) %>% dplyr::group_by(domain) %>% mutate(perc = team_name/sum(team_name)*100)%>% arrange(-Domain_Publications) %>% filter(Domain_Publications ==1)

proportions(xtabs( ~ domain+Domain_Publications,phase1_exp),"domain")*100 #by domain

proportions(xtabs( ~ domain+Domain_Publications,dat_phase2),"domain")*100 #by domain


#age
psych::describe(phase1.exp.dem$team_Age)
psych::describe(phase2.exp.dem$team_Age)

#Percentage of team members who indicated their level of education was not PhD
psych::describe(phase1.exp.dem$team_education)
psych::describe(phase2.exp.dem$team_education)

#to get number of team members with a PhD

##multiple % per team without a PhD by number of team members and subtract from 1 and multiple by 100 to get % of forecasts done by PhDs
(1 - sum(phase1.exp.dem$team_education/100*phase1.exp.dem$team_size.coded)/sum(phase1.exp.dem$team_size.coded))*100 #73.33%

(1 - sum(phase2.exp.dem$team_education/100*phase2.exp.dem$team_size.coded)/sum(phase2.exp.dem$team_size.coded))*100 #67.37%

# non-US Percentage of team members who indicated their country of residence was not the United States
psych::describe(phase1.exp.dem$non_US)
psych::describe(phase2.exp.dem$non_US)

(sum(phase1.exp.dem$non_US/100*phase1.exp.dem$team_size.coded)/sum(phase1.exp.dem$team_size.coded))*100 #62.22% nonus

(sum(phase2.exp.dem$non_US/100*phase2.exp.dem$team_size.coded)/sum(phase2.exp.dem$team_size.coded))*100 #62.10526% nonus

#gender: Percentage of team members who indicated their gender was either Female or Other
psych::describe(phase1.exp.dem$team_gender)
psych::describe(phase2.exp.dem$team_gender)

#male phase 1
(1 - sum(phase1.exp.dem$team_gender/100*phase1.exp.dem$team_size.coded)/sum(phase1.exp.dem$team_size.coded))*100 #76.2963% male

(1 - sum(phase2.exp.dem$team_gender/100*phase2.exp.dem$team_size.coded)/sum(phase2.exp.dem$team_size.coded))*100 #74.47368% male

#% revised predictions

psych::describe(phase1.exp.dem$revised*100)

#did preference for updating vary by method?
model.revised.t1.by.method<-glmer(revised~method.contrast+(1|team_name), data=phase1_exp, family=binomial)
summ(model.revised.t1.by.method, conf.method="boot", digits=3, center=T)

#theory vs. intuition in each phase
table(phase1_exp$basis)

```

# demographics lay people

```{r}
phase1.noexp.dem<-phase1 %>% filter(isExpert.factor=="Prolific") %>% dplyr::select(ResponseId,Age, Sex, Ethnicity, Education,Residential.Area, Income) %>% dplyr::group_by(ResponseId) %>% 
summarize_all(~ mean(.x, na.rm = TRUE))
psych::describe(phase1.noexp.dem$Age)
prop.table(table(phase1.noexp.dem$Sex)) #46.36% female
prop.table(table(phase1.noexp.dem$Education))
#"1 =  less than highschool
#2 =  highschool grad
#3 =  some college
#4 =  vocational/technical degree
#5 =  bachelor's
#6 =  masters'
#7 =  doctorate
#8 =  professional degree"
0.416846652+ 0.162706983+ 0.017278618+ 0.029517639 #proportion with a college degree or above
0.003599712+ 0.082073434 #high school or less
0.245500360+ 0.042476602 #some college
prop.table(table(phase1.noexp.dem$Ethnicity))
#1-9 ranging from aboriginal/native, asian, black, white, middle eastern, hispanic, east indian, mixed race, other/not listed
prop.table(table(phase1.noexp.dem$Income))
#from 0-15k, 15-25k, 25-35k, 35-50k, 50-75k, 75-100k,100-150k, 150k+
prop.table(table(phase1.noexp.dem$Residential.Area))
#"1 = Urban,
#2 = Suburban
#3 = rural"

```

